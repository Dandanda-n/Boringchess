{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9504094,"sourceType":"datasetVersion","datasetId":5784445},{"sourceId":9928129,"sourceType":"datasetVersion","datasetId":6102415},{"sourceId":9928267,"sourceType":"datasetVersion","datasetId":6102496},{"sourceId":9928973,"sourceType":"datasetVersion","datasetId":6102975},{"sourceId":9956315,"sourceType":"datasetVersion","datasetId":6123422},{"sourceId":9987553,"sourceType":"datasetVersion","datasetId":5755794},{"sourceId":9994467,"sourceType":"datasetVersion","datasetId":6151392}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"        # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate a requirements.txt file for the Kaggle notebook environment\n\nrequirements = \"\"\"paddlepaddle-gpu>=2.4.2\npaddleocr>=2.6.0\ntensorflow>=2.10.0\nnumpy>=1.21.0\npillow>=8.0.0\n\"\"\"\n\n# Save the requirements.txt file in the current working directory\nrequirements_file_path = \"/kaggle/working/requirements.txt\"\n\nwith open(requirements_file_path, \"w\") as file:\n    file.write(requirements)\n\nprint(f\"Requirements file saved at {requirements_file_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -r /kaggle/working/requirements.txt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"test extracting text box coordinates","metadata":{}},{"cell_type":"markdown","source":"try box ocr,image classification, ocr","metadata":{}},{"cell_type":"code","source":"#(not for danny-chess-2) use this code to generate labels and annotated images. In order for labels to be valid, they will have to be reviewed by human and mistakes corrected only in labels column\nimport os\nimport cv2\nfrom paddleocr import PaddleOCR\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Define input and output paths\ninput_folder = '/kaggle/input/chess-dataset-notation/data'\noutput_folder = '/kaggle/working/labels'\nannotated_folder = '/kaggle/working/annotated_images'\nprinted_folder = os.path.join(output_folder, 'printed')\nhandwritten_folder = os.path.join(output_folder, 'handwritten')\n\n# Create necessary directories\nos.makedirs(printed_folder, exist_ok=True)\nos.makedirs(handwritten_folder, exist_ok=True)\nos.makedirs(annotated_folder, exist_ok=True)\n\n# Initialize PaddleOCR\nocr = PaddleOCR(\n    use_angle_cls=True,\n    lang='en',\n    det_db_box_thresh=0.4,\n    det_db_unclip_ratio=1.1,\n    drop_score=0.3,\n    rec_image_shape=\"3, 48, 300\",\n    det_limit_side_len=2400\n)\n\n# Fixed colors for each column\nCOLUMN_COLORS = {\n    1: \"blue\",    # Column 1 - Printed\n    2: \"red\",     # Column 2 - Handwritten\n    3: \"yellow\",  # Column 3 - Handwritten\n    4: \"green\",   # Column 4 - Printed\n    5: \"orange\",  # Column 5 - Handwritten\n    6: \"purple\"   # Column 6 - Handwritten\n}\n\n# Helper function to classify the column and get its color\ndef classify_column(x_center, image_width):\n    first_column_end = image_width / 15\n    fourth_column_start = image_width / 2\n    fourth_column_end = fourth_column_start + image_width / 20\n    column_width = (image_width - first_column_end - (fourth_column_end - fourth_column_start) - (image_width / 20)) / 4\n\n    if 0 <= x_center < first_column_end:\n        return \"printed\", COLUMN_COLORS[1]\n    elif first_column_end <= x_center < first_column_end + column_width:\n        return \"handwritten\", COLUMN_COLORS[2]\n    elif first_column_end + column_width <= x_center < first_column_end + 2 * column_width:\n        return \"handwritten\", COLUMN_COLORS[3]\n    elif fourth_column_start <= x_center < fourth_column_end:\n        return \"printed\", COLUMN_COLORS[4]\n    elif fourth_column_end <= x_center < fourth_column_end + column_width:\n        return \"handwritten\", COLUMN_COLORS[5]\n    elif fourth_column_end + column_width <= x_center < fourth_column_end + 2 * column_width:\n        return \"handwritten\", COLUMN_COLORS[6]\n    return None, None\n\n# Process each PNG file in the input folder\nfor filename in os.listdir(input_folder):\n    if filename.endswith('.png'):\n        input_image_path = os.path.join(input_folder, filename)\n        preprocessed_image_path = f'/kaggle/working/preprocessed_{filename}'\n\n        # Step 1: Pre-Process the Image\n        image = cv2.imread(input_image_path)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        filtered = cv2.bilateralFilter(gray, 9, 75, 75)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        contrast_enhanced = clahe.apply(filtered)\n        cv2.imwrite(preprocessed_image_path, contrast_enhanced)\n\n        # Step 2: Perform OCR\n        ocr_result = ocr.ocr(preprocessed_image_path, cls=True)\n\n        # Step 3: Annotate the Original Image\n        annotated_image = Image.open(input_image_path).convert(\"RGB\")  # Convert to RGB mode\n        draw = ImageDraw.Draw(annotated_image)\n        font = ImageFont.load_default()\n\n        # Define top and bottom margins\n        top_margin = 25  # Ignore boxes above this Y-coordinate\n        bottom_margin = annotated_image.height - 25  # Ignore boxes below this Y-coordinate\n\n        # Step 4: Process OCR Results\n        image_width = annotated_image.width\n        box_number = 1\n\n        for line_num, line in enumerate(ocr_result[0]):\n            bbox = line[0]\n            x_min = int(min(point[0] for point in bbox))\n            y_min = int(min(point[1] for point in bbox))\n            x_max = int(max(point[0] for point in bbox))\n            y_max = int(max(point[1] for point in bbox))\n            x_center = (x_min + x_max) / 2\n            y_center = (y_min + y_max) / 2\n\n            # Apply top and bottom margin filtering\n            if y_center < top_margin or y_center > bottom_margin:\n                continue\n\n            # Classify the column\n            label, color = classify_column(x_center, image_width)\n            if label:\n                # Crop the text region\n                cropped = image[y_min:y_max, x_min:x_max]\n\n                # Save the cropped region\n                output_subfolder = printed_folder if label == \"printed\" else handwritten_folder\n                output_path = os.path.join(output_subfolder, f\"{filename}_box{box_number}.png\")\n                cv2.imwrite(output_path, cropped)\n\n                # Annotate the image\n                int_bbox = [(int(point[0]), int(point[1])) for point in bbox]\n                draw.polygon(int_bbox, outline=color)\n                draw.text((x_min, y_min), f\"{box_number}\", fill=color)\n                box_number += 1\n\n        # Draw top and bottom margin lines on the annotated image\n        draw.line([(0, top_margin), (image_width, top_margin)], fill=\"green\", width=2)  # Top margin\n        draw.line([(0, bottom_margin), (image_width, bottom_margin)], fill=\"green\", width=2)  # Bottom margin\n\n        # Save the annotated image\n        annotated_image_path = os.path.join(annotated_folder, f\"annotated_{filename}\")\n        annotated_image.save(annotated_image_path)\n\nprint(f\"Labeled data saved in: {output_folder}\")\nprint(f\"Annotated images saved in: {annotated_folder}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:49:59.269599Z","iopub.execute_input":"2024-11-24T17:49:59.270423Z","iopub.status.idle":"2024-11-24T17:50:00.924903Z","shell.execute_reply.started":"2024-11-24T17:49:59.270379Z","shell.execute_reply":"2024-11-24T17:50:00.923564Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[2024/11/24 17:49:59] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=2400, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.4, det_db_unclip_ratio=1.1, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/opt/conda/lib/python3.10/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.3, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Process each PNG file in the input folder\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     64\u001b[0m         input_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, filename)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/chess-dataset-notation/data/testing'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/chess-dataset-notation/data/testing'","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"get coordinates of the cells","metadata":{}},{"cell_type":"code","source":"#line annotations. outdated\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\n\n# Load the uploaded image\nimage_path = '/kaggle/input/danny-chess-2/2024-11-23 15-35.jpeg'\nimage = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n# Apply edge detection to identify lines\nedges = cv2.Canny(image, 50, 150, apertureSize=3)\n\n# Use Hough Line Transform to detect lines\nlines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=50, maxLineGap=10)\n\n# Store all line segments for intersection calculation\nhorizontal_lines = []\nvertical_lines = []\n\n# Classify detected lines into horizontal and vertical\nfor line in lines:\n    for x1, y1, x2, y2 in line:\n        if abs(y2 - y1) < 5:  # Horizontal line\n            horizontal_lines.append((x1, y1, x2, y2))\n        elif abs(x2 - x1) < 5:  # Vertical line\n            vertical_lines.append((x1, y1, x2, y2))\n\n# Find intersections between horizontal and vertical lines\nintersections = []\nfor h_line in horizontal_lines:\n    for v_line in vertical_lines:\n        x1_h, y1_h, x2_h, y2_h = h_line\n        x1_v, y1_v, x2_v, y2_v = v_line\n\n        # Check if the vertical line crosses the horizontal line\n        if min(x1_h, x2_h) <= x1_v <= max(x1_h, x2_h) and min(y1_v, y2_v) <= y1_h <= max(y1_v, y2_v):\n            intersections.append((x1_v, y1_h))\n\n# Sort intersections for consistent box creation\nintersections = sorted(intersections, key=lambda x: (x[1], x[0]))  # Sort by y, then x\n\n# Create boxes only for adjacent points\nbox_id = 1  # Box numbering starts from 1\nfor i in range(len(intersections)):\n    x1, y1 = intersections[i]\n\n    # Find the points that are adjacent horizontally and vertically\n    for j in range(len(intersections)):\n        if i != j:\n            x2, y2 = intersections[j]\n\n            # Ensure valid horizontal or vertical adjacency\n            if (abs(x2 - x1) > 0 and abs(y2 - y1) == 0) or (abs(y2 - y1) > 0 and abs(x2 - x1) == 0):\n                # Create rectangle only when two pairs form a valid box\n                if (x2, y2) in intersections and (x1, y2) in intersections and (x2, y1) in intersections:\n                    # Top-left and bottom-right points of the box\n                    top_left = (x1, y1)\n                    bottom_right = (x2, y2)\n\n                    # Draw the green rectangle\n                    cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n\n                    # Add box number at the center\n                    center_x = (x1 + x2) // 2\n                    center_y = (y1 + y2) // 2\n                    cv2.putText(image, str(box_id), (center_x - 10, center_y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n                    # Increment the box ID and break to avoid overlaps\n                    box_id += 1\n                    break\n\n# Define the path to save the annotated image\noutput_dir = '/kaggle/working/annotated_images'\nos.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\noutput_path = os.path.join(output_dir, '2024-11-23 15-35.jpeg')\n\n# Save the annotated image\ncv2.imwrite(output_path, image)\n\nprint(f\"Annotated image saved to {output_path}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -r /kaggle/working/labels_yolo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create labels for the dataset for incpetionv3 classification","metadata":{}},{"cell_type":"code","source":"#(not for danny-chess-2) labels and annotation files intended for image classification not ocr\nimport os\nimport cv2\nfrom paddleocr import PaddleOCR\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Define input and output paths\ninput_folder = '/kaggle/input/chess-dataset-notation/data'\noutput_folder = '/kaggle/working/labels'\nannotated_folder = '/kaggle/working/annotated_images'\nprinted_folder = os.path.join(output_folder, 'printed')\nhandwritten_folder = os.path.join(output_folder, 'handwritten')\n\n# Create necessary directories\nos.makedirs(printed_folder, exist_ok=True)\nos.makedirs(handwritten_folder, exist_ok=True)\nos.makedirs(annotated_folder, exist_ok=True)\n\n# Initialize PaddleOCR\nocr = PaddleOCR(\n    use_angle_cls=True,\n    lang='en',\n    det_db_box_thresh=0.4,\n    det_db_unclip_ratio=1.1,\n    drop_score=0.3,\n    rec_image_shape=\"3, 48, 300\",\n    det_limit_side_len=2400\n)\n\n# Predefined palette colors\nPREDEFINED_COLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]  # Red, Green, Blue, Yellow\n\n# Helper function to get colors based on box number\ndef get_color(box_number):\n    return PREDEFINED_COLORS[box_number % len(PREDEFINED_COLORS)]\n\n# Helper function to classify the column\ndef classify_column(x_center, image_width):\n    first_column_end = image_width / 15\n    fourth_column_start = image_width / 2\n    fourth_column_end = fourth_column_start + image_width / 20\n    column_width = (image_width - first_column_end - (fourth_column_end - fourth_column_start) - (image_width / 20)) / 4\n\n    if 0 <= x_center < first_column_end:\n        return \"printed\"\n    elif first_column_end <= x_center < first_column_end + column_width:\n        return \"handwritten\"\n    elif first_column_end + column_width <= x_center < first_column_end + 2 * column_width:\n        return \"handwritten\"\n    elif fourth_column_start <= x_center < fourth_column_end:\n        return \"printed\"\n    elif fourth_column_end <= x_center < fourth_column_end + column_width:\n        return \"handwritten\"\n    elif fourth_column_end + column_width <= x_center < fourth_column_end + 2 * column_width:\n        return \"handwritten\"\n    return None\n\n# Process each PNG file in the input folder\nfor filename in os.listdir(input_folder):\n    if filename.endswith('.png'):\n        input_image_path = os.path.join(input_folder, filename)\n        preprocessed_image_path = f'/kaggle/working/preprocessed_{filename}'\n\n        # Step 1: Pre-Process the Image\n        image = cv2.imread(input_image_path)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        filtered = cv2.bilateralFilter(gray, 9, 75, 75)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        contrast_enhanced = clahe.apply(filtered)\n        cv2.imwrite(preprocessed_image_path, contrast_enhanced)\n\n        # Step 2: Perform OCR\n        ocr_result = ocr.ocr(preprocessed_image_path, cls=True)\n\n        # Step 3: Annotate the Original Image\n        annotated_image = Image.open(input_image_path).convert(\"P\")  # Keep P mode\n        draw = ImageDraw.Draw(annotated_image)\n        font = ImageFont.load_default()\n\n        # Step 4: Process OCR Results\n        image_width = annotated_image.width\n        box_number = 1\n\n        for line_num, line in enumerate(ocr_result[0]):\n            bbox = line[0]\n            x_min = int(min(point[0] for point in bbox))\n            y_min = int(min(point[1] for point in bbox))\n            x_max = int(max(point[0] for point in bbox))\n            y_max = int(max(point[1] for point in bbox))\n            x_center = (x_min + x_max) / 2\n\n            # Classify the column\n            label = classify_column(x_center, image_width)\n            if label:\n                # Crop the text region\n                cropped = image[y_min:y_max, x_min:x_max]\n\n                # Save the cropped region\n                output_subfolder = printed_folder if label == \"printed\" else handwritten_folder\n                output_path = os.path.join(output_subfolder, f\"{filename}_box{box_number}.png\")\n                cv2.imwrite(output_path, cropped)\n\n                # Annotate the image\n                int_bbox = [(int(point[0]), int(point[1])) for point in bbox]\n                color = get_color(box_number)  # Get color for this box\n                draw.polygon(int_bbox, outline=color)\n                draw.text((x_min, y_min), f\"{box_number}\", fill=color)\n                box_number += 1\n\n        # Save the annotated image\n        annotated_image_path = os.path.join(annotated_folder, f\"annotated_{filename}\")\n        annotated_image.save(annotated_image_path)\n\nprint(f\"Labeled data saved in: {output_folder}\")\nprint(f\"Annotated images saved in: {annotated_folder}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#box labeling for yolo. expiremental\nimport os\nimport cv2\nfrom paddleocr import PaddleOCR\n\n# Define input and output paths\ninput_folder = '/kaggle/input/chess-dataset-notation/data'\noutput_folder = '/kaggle/working/labels_yolo'\nimages_folder = os.path.join(output_folder, 'images')\nlabels_folder = os.path.join(output_folder, 'labels')\n\n# Create necessary directories\nos.makedirs(images_folder, exist_ok=True)\nos.makedirs(labels_folder, exist_ok=True)\n\n# Initialize PaddleOCR\nocr = PaddleOCR(\n    use_angle_cls=False,\n    lang='en',\n    det_db_box_thresh=0.4,\n    det_db_unclip_ratio=1.1,\n    drop_score=0.3,\n    rec_image_shape=\"3, 48, 300\",\n    det_limit_side_len=2400\n)\n\n# Class IDs for YOLOv8\nCLASS_IDS = {\n    \"printed\": 0,      # Class ID 0 for printed text\n    \"handwritten\": 1   # Class ID 1 for handwritten text\n}\n\n# Helper function to classify the column\ndef classify_column(x_center, image_width):\n    first_column_end = image_width / 15\n    fourth_column_start = image_width / 2\n    fourth_column_end = fourth_column_start + image_width / 20\n    column_width = (image_width - first_column_end - (fourth_column_end - fourth_column_start) - (image_width / 20)) / 4\n\n    if 0 <= x_center < first_column_end:\n        return \"printed\"\n    elif first_column_end <= x_center < first_column_end + column_width:\n        return \"handwritten\"\n    elif first_column_end + column_width <= x_center < first_column_end + 2 * column_width:\n        return \"handwritten\"\n    elif fourth_column_start <= x_center < fourth_column_end:\n        return \"printed\"\n    elif fourth_column_end <= x_center < fourth_column_end + column_width:\n        return \"handwritten\"\n    elif fourth_column_end + column_width <= x_center < fourth_column_end + 2 * column_width:\n        return \"handwritten\"\n    return None\n\n# Process each PNG file in the input folder\nfor filename in os.listdir(input_folder):\n    if filename.endswith('.png'):\n        input_image_path = os.path.join(input_folder, filename)\n\n        # Step 1: Pre-Process the Image\n        image = cv2.imread(input_image_path)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        filtered = cv2.bilateralFilter(gray, 9, 75, 75)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        contrast_enhanced = clahe.apply(filtered)\n\n        # Save the preprocessed image in the images folder\n        output_image_path = os.path.join(images_folder, filename)\n        cv2.imwrite(output_image_path, contrast_enhanced)\n\n        # Perform OCR on the preprocessed image\n        ocr_result = ocr.ocr(output_image_path, cls=True)\n\n        # Define top and bottom margins\n        image_height, image_width = contrast_enhanced.shape\n        top_margin = 25  # Ignore boxes above this Y-coordinate\n        bottom_margin = image_height - 25  # Ignore boxes below this Y-coordinate\n\n        # Initialize label file\n        label_file_path = os.path.join(labels_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n        with open(label_file_path, 'w') as label_file:\n            for line in ocr_result[0]:\n                bbox = line[0]\n                x_min = min(point[0] for point in bbox)\n                y_min = min(point[1] for point in bbox)\n                x_max = max(point[0] for point in bbox)\n                y_max = max(point[1] for point in bbox)\n                x_center = (x_min + x_max) / 2\n                y_center = (y_min + y_max) / 2\n\n                # Skip boxes outside the margins\n                if y_center < top_margin or y_center > bottom_margin:\n                    continue\n\n                # Normalize coordinates\n                x_center_norm = x_center / image_width\n                y_center_norm = y_center / image_height\n                width_norm = (x_max - x_min) / image_width\n                height_norm = (y_max - y_min) / image_height\n\n                # Classify the column and write to the label file\n                label = classify_column(x_center, image_width)\n                if label:\n                    class_id = CLASS_IDS[label]\n                    label_file.write(f\"{class_id} {x_center_norm:.6f} {y_center_norm:.6f} {width_norm:.6f} {height_norm:.6f}\\n\")\n\nprint(f\"Images and labels saved in: {output_folder}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#some inceptionv3 testing. Not in use\ninception_model = tf.keras.models.load_model('/kaggle/input/your-dataset-name/handwritten_classifier.h5')\n\n# Step 5: Run the Pipeline on an Image\nimage_path = '/kaggle/input/chess-dataset-notation/data/011_0.png'\nhandwritten_texts = ocr_handwritten_boxes(image_path)\n\n# Output the handwritten text\nprint(\"Handwritten Text Extracted:\")\nfor text in handwritten_texts:\n    print(text)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the file into a Python list\nwith open('/kaggle/input/valid-moves/san_strings_with_symbols.txt', 'r') as file:\n    chess_notations = file.read().splitlines()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#some training imports for metrics calculations\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the labels file into a DataFrame(\ndataset_path = '/kaggle/input/chess-dataset-notation/data'\nlabels_file = os.path.join(dataset_path, 'training_tags.txt')\nlabels_df = pd.read_csv(labels_file, sep=' ', header=None, names=['filename', 'label'])\n\n# Add full paths to filenames\nlabels_df['filepath'] = labels_df['filename'].apply(lambda x: os.path.join(dataset_path, x))\nprint(labels_df)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#bad training labels\nfrom sklearn.model_selection import train_test_split\n\ntraining_labels_file = os.path.join(dataset_path, 'training_tags.txt')\ntesting_labels_file = os.path.join(dataset_path, 'testing_tags.txt')\n\n# Load training labels\ntraining_labels_df = pd.read_csv(training_labels_file, sep=' ', header=None, names=['filename', 'label'])\ntraining_labels_df['filepath'] = training_labels_df['filename'].apply(lambda x: os.path.join(dataset_path, x))\n\n# Load testing labels\ntesting_labels_df = pd.read_csv(testing_labels_file, sep=' ', header=None, names=['filename', 'label'])\ntesting_labels_df['filepath'] = testing_labels_df['filename'].apply(lambda x: os.path.join(dataset_path, x))\n\n\n# Filter out classes with fewer than 2 instances\nfiltered_labels_df = training_labels_df.groupby('label').filter(lambda x: len(x) > 1)\n\n# Perform the train-test split\ntrain_df, val_df = train_test_split(\n    f\n    iltered_labels_df,\n    test_size=0.1,\n    random_state=42,\n    stratify=filtered_labels_df['label']\n)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load chess-dataset-notation training tags\nimport os\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport os\n\n# Base directory for data\ndataset_path = '/kaggle/input/chess-dataset-notation/data'\n\n# Paths for images and labels\nimage_path = dataset_path  # Images are in the same folder\ntraining_labels_file = os.path.join(dataset_path, 'training_tags.txt')\n    ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone the YOLOv5 repository\n!git clone https://github.com/ultralytics/yolov5\n%cd yolov5\n\n# Install requirements\n!pip install -r requirements.txt\n!pip install torch torchvision pillow","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 -m pip install paddlepaddle-gpu==2.4.2.post112 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m pip install paddlepaddle==2.4.2 -i https://mirror.baidu.com/pypi/simple","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import paddle\npaddle.utils.run_check()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/PaddlePaddle/PaddleOCR.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tqdm imutils opencv-python matplotlib gdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#download PPOCRdata.tar\nimport gdown\n\nurl = \"https://drive.google.com/uc?id=19kSj_GswccuXk45yH87jTXLqpLTxaS67\"\noutput = \"PPOCRdata.tar\"\n\ngdown.download(url, output)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh PaddleOCR","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 PaddleOCR/setup.py --help-commands","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -xf PPOCRdata.tar && rm -rf PPOCRdata.tar","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2 \nimport os \n#from imutils import perspective\nimport numpy as np \nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom imutils import perspective","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    !pip install paddlepaddle\n    !pip install paddleocr\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:19:00.874130Z","iopub.execute_input":"2024-11-24T16:19:00.874583Z","iopub.status.idle":"2024-11-24T16:19:46.891340Z","shell.execute_reply.started":"2024-11-24T16:19:00.874536Z","shell.execute_reply":"2024-11-24T16:19:46.890281Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting paddlepaddle\n  Downloading paddlepaddle-2.6.2-cp310-cp310-manylinux1_x86_64.whl.metadata (8.6 kB)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from paddlepaddle) (0.27.0)\nRequirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.10/site-packages (from paddlepaddle) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from paddlepaddle) (9.5.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from paddlepaddle) (5.1.1)\nCollecting astor (from paddlepaddle)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: opt-einsum==3.3.0 in /opt/conda/lib/python3.10/site-packages (from paddlepaddle) (3.3.0)\nRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from paddlepaddle) (3.20.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->paddlepaddle) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->paddlepaddle) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->paddlepaddle) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->paddlepaddle) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->paddlepaddle) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->paddlepaddle) (1.2.0)\nRequirement already satisfied: typing-extensions>=4.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->paddlepaddle) (4.12.2)\nDownloading paddlepaddle-2.6.2-cp310-cp310-manylinux1_x86_64.whl (126.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nInstalling collected packages: astor, paddlepaddle\nSuccessfully installed astor-0.8.1 paddlepaddle-2.6.2\nCollecting paddleocr\n  Downloading paddleocr-2.9.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: shapely in /opt/conda/lib/python3.10/site-packages (from paddleocr) (1.8.5.post1)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from paddleocr) (0.23.2)\nRequirement already satisfied: imgaug in /opt/conda/lib/python3.10/site-packages (from paddleocr) (0.4.0)\nRequirement already satisfied: pyclipper in /opt/conda/lib/python3.10/site-packages (from paddleocr) (1.3.0.post5)\nCollecting lmdb (from paddleocr)\n  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from paddleocr) (4.66.4)\nRequirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from paddleocr) (1.26.4)\nCollecting rapidfuzz (from paddleocr)\n  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from paddleocr) (4.10.0.84)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from paddleocr) (4.10.0.84)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from paddleocr) (3.0.10)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from paddleocr) (9.5.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from paddleocr) (6.0.2)\nCollecting python-docx (from paddleocr)\n  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from paddleocr) (4.12.3)\nRequirement already satisfied: fonttools>=4.24.0 in /opt/conda/lib/python3.10/site-packages (from paddleocr) (4.53.0)\nCollecting fire>=0.3.0 (from paddleocr)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from paddleocr) (2.32.3)\nCollecting albumentations==1.4.10 (from paddleocr)\n  Downloading albumentations-1.4.10-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: albucore==0.0.13 in /opt/conda/lib/python3.10/site-packages (from paddleocr) (0.0.13)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from albucore==0.0.13->paddleocr) (2.0.1)\nRequirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from albucore==0.0.13->paddleocr) (4.12.2)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/conda/lib/python3.10/site-packages (from albucore==0.0.13->paddleocr) (4.10.0.84)\nRequirement already satisfied: scipy>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.4.10->paddleocr) (1.14.0)\nCollecting scikit-learn>=1.3.2 (from albumentations==1.4.10->paddleocr)\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: pydantic>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.4.10->paddleocr) (2.8.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire>=0.3.0->paddleocr) (2.4.0)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->paddleocr) (3.3)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->paddleocr) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->paddleocr) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->paddleocr) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->paddleocr) (0.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->paddleocr) (2.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from imgaug->paddleocr) (1.16.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from imgaug->paddleocr) (3.7.5)\nRequirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from python-docx->paddleocr) (5.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->paddleocr) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->paddleocr) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->paddleocr) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->paddleocr) (2024.7.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->paddleocr) (3.1.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (2.20.1)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->paddleocr) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->paddleocr) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug->paddleocr) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug->paddleocr) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug->paddleocr) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug->paddleocr) (2.9.0.post0)\nDownloading paddleocr-2.9.1-py3-none-any.whl (544 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.7/544.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading albumentations-1.4.10-py3-none-any.whl (161 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=585435ba9096774d47f6cf4d34b9b3aaec4e76a52041a1e9d50af5360f114e92\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built fire\nInstalling collected packages: lmdb, rapidfuzz, python-docx, fire, scikit-learn, albumentations, paddleocr\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.4.14\n    Uninstalling albumentations-1.4.14:\n      Successfully uninstalled albumentations-1.4.14\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed albumentations-1.4.10 fire-0.7.0 lmdb-1.5.1 paddleocr-2.9.1 python-docx-1.1.2 rapidfuzz-3.10.1 scikit-learn-1.5.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# ****extract text using paddleocr and save annotated file","metadata":{}},{"cell_type":"code","source":"#use this code to generate labels and annotation files for danny-chess-2\nfrom paddleocr import PaddleOCR\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport csv\nimport json\n\n# Parameters\nINPUT_FOLDER = '/kaggle/input/danny-chess-2/testing'  # Set input folder path\nLABELS_FOLDER = '/kaggle/working/paddle_labels'\nANNOTATED_IMAGES_FOLDER = '/kaggle/working/paddle_annotated'\nFONT_SIZE = 20  # Font size for annotations\nBOX_MARGIN = 30  # Margin for expanding box coordinates\nANNOTATION_COLOR = \"green\"  # Color for annotations\n\n# Initialize PaddleOCR with original parameters\nocr = PaddleOCR(\n    use_angle_cls=False,           # Disable angle classification to detect and correct text orientation. Default: False\n    lang='en',                     # Use the English language model for OCR. Default: 'en'\n    det_db_box_thresh=0.4,         # Threshold for text box detection confidence; lower values may detect more boxes but increase noise. Default: 0.4\n    det_db_unclip_ratio=1.5,       # Expansion ratio for detected text boxes; smaller values create tighter boxes around text. Default: 1.5\n    drop_score=0.4,                # Minimum confidence score for retaining detected text; lower values allow less certain text results. Default: 0.5\n    rec_image_shape=\"3, 48, 320\",  # Input shape for the recognition model; specifies channels, height, and width of the input image. Default: \"3, 48, 320\"\n    det_limit_side_len=960         # Maximum side length of the input image for detection; larger values allow processing of larger images. Default: 960\n)\n\n# Ensure output folders exist\nos.makedirs(LABELS_FOLDER, exist_ok=True)\nos.makedirs(ANNOTATED_IMAGES_FOLDER, exist_ok=True)\n\n# Process all JPEG/JPG files in the input folder\nfor filename in os.listdir(INPUT_FOLDER):\n    # Only process .jpeg or .jpg files\n    if filename.lower().endswith(('.jpeg', '.jpg')):\n        image_path = os.path.join(INPUT_FOLDER, filename)\n        csv_path = os.path.join(LABELS_FOLDER, f\"{filename}.csv\")\n\n        # Skip if the corresponding CSV file already exists\n        if os.path.exists(csv_path):\n            print(f\"Skipping {filename} - CSV file already exists.\")\n            continue\n\n        print(f\"Processing {filename}...\")\n\n        # Perform OCR\n        result = ocr.ocr(image_path, cls=True)\n\n        # Load the image for annotation\n        image = Image.open(image_path).convert('RGB')\n        draw = ImageDraw.Draw(image)\n\n        # Load font\n        try:\n            font = ImageFont.truetype(\"arial.ttf\", size=FONT_SIZE)\n        except IOError:\n            font = ImageFont.load_default()\n\n        # Extract white and black ranges\n        white_ranges, black_ranges = [], []\n        for res in result:\n            for line in res:\n                box = line[0]\n                text = line[1][0].lower()\n                x_min, x_max = box[0][0] - BOX_MARGIN, box[2][0] + BOX_MARGIN\n                if text == \"white\":\n                    white_ranges.append((x_min, x_max))\n                elif text == \"black\":\n                    black_ranges.append((x_min, x_max))\n\n        # Define helper functions\n        def calculate_x_center(box):\n            return (box[0][0] + box[2][0]) / 2\n\n        def is_within_any_range(x_center, ranges):\n            return any(x_min <= x_center <= x_max for x_min, x_max in ranges)\n\n        # Annotate image and save results\n        preserved_boxes = []\n        box_number = 1\n        csv_data = []\n        for res in result:\n            for line in res:\n                box = line[0]\n                x_center = calculate_x_center(box)\n                confidence = line[1][1] * 100\n                text = line[1][0]\n\n                # Draw annotations\n                # Ensure coordinates are in the correct order\n                x_min = int(min(box[0][0], box[2][0]))\n                y_min = int(min(box[0][1], box[2][1]))\n                x_max = int(max(box[0][0], box[2][0]))\n                y_max = int(max(box[0][1], box[2][1]))\n                \n                # Draw annotations\n                draw.rectangle([x_min, y_min, x_max, y_max], outline=ANNOTATION_COLOR, width=2)\n                draw.text((x_min, y_min - 10), str(box_number), fill=ANNOTATION_COLOR, font=font)\n                draw.rectangle([x_min, y_min, x_max, y_max], outline=ANNOTATION_COLOR, width=2)\n                draw.text((x_min, y_min - 10), str(box_number), fill=ANNOTATION_COLOR, font=font)\n\n                # Check ranges\n                if is_within_any_range(x_center, white_ranges) or is_within_any_range(x_center, black_ranges):\n                    preserved_boxes.append({\n                        \"box_number\": box_number,\n                        \"x_center\": x_center,\n                        \"text\": text,\n                        \"confidence\": confidence,\n                        \"coordinates\": box\n                    })\n                    csv_data.append([box_number, confidence, json.dumps(box), text, text])\n                    print(f\"Box {box_number}, x-center: {x_center}, Text: {text}, Confidence: {confidence:.2f}%\")\n                \n                box_number += 1\n\n        # Save annotated image\n        annotated_img_path = os.path.join(ANNOTATED_IMAGES_FOLDER, filename)\n        image.save(annotated_img_path)\n        print(f\"Annotated image saved at: {annotated_img_path}\")\n\n        # Save CSV\n        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\"Box Number\", \"Confidence\", \"Box Coordinates\", \"Original Text\", \"Label\"])\n            csvwriter.writerows(csv_data)\n\n        print(f\"Label CSV file saved at: {csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:53:53.464663Z","iopub.execute_input":"2024-11-24T17:53:53.465328Z","iopub.status.idle":"2024-11-24T17:54:26.791607Z","shell.execute_reply.started":"2024-11-24T17:53:53.465283Z","shell.execute_reply":"2024-11-24T17:54:26.790604Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[2024/11/24 17:53:53] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.4, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/opt/conda/lib/python3.10/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.4, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\nProcessing 20241124-1012.jpg...\n[2024/11/24 17:53:54] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n[2024/11/24 17:53:54] ppocr DEBUG: dt_boxes num : 100, elapsed : 0.3274376392364502\n[2024/11/24 17:53:58] ppocr DEBUG: rec_res num  : 100, elapsed : 3.728882312774658\nBox 1, x-center: 298.5, Text: OFFICIAL, Confidence: 99.47%\nBox 13, x-center: 356.0, Text: WHITE, Confidence: 99.75%\nBox 14, x-center: 640.5, Text: BLACK, Confidence: 99.70%\nBox 15, x-center: 991.5, Text: WHITE, Confidence: 99.77%\nBox 16, x-center: 1278.0, Text: BLACK, Confidence: 99.69%\nBox 17, x-center: 308.5, Text: c4, Confidence: 91.98%\nBox 18, x-center: 591.0, Text: Ne6, Confidence: 89.92%\nBox 20, x-center: 328.5, Text: Nc3, Confidence: 94.58%\nBox 21, x-center: 587.0, Text: Nf6, Confidence: 92.94%\nBox 23, x-center: 579.5, Text: d5, Confidence: 96.24%\nBox 24, x-center: 322.0, Text: ey, Confidence: 76.42%\nBox 26, x-center: 312.5, Text: e5, Confidence: 99.33%\nBox 27, x-center: 586.0, Text: d4, Confidence: 96.77%\nBox 29, x-center: 368.0, Text: exf6, Confidence: 85.56%\nBox 30, x-center: 619.5, Text: dxc3, Confidence: 94.64%\nBox 32, x-center: 332.5, Text: bxc3, Confidence: 91.43%\nBox 33, x-center: 644.0, Text: f6, Confidence: 63.28%\nBox 35, x-center: 323.5, Text: Nf3, Confidence: 95.49%\nBox 36, x-center: 577.0, Text: c5, Confidence: 90.56%\nBox 38, x-center: 322.0, Text: d4, Confidence: 97.56%\nBox 39, x-center: 571.0, Text: h6, Confidence: 80.52%\nBox 41, x-center: 322.5, Text: Bd3, Confidence: 98.84%\nBox 42, x-center: 595.5, Text: Nc6, Confidence: 85.53%\nBox 44, x-center: 320.0, Text: Bb2, Confidence: 93.69%\nBox 45, x-center: 617.0, Text: Bd6, Confidence: 77.66%\nBox 48, x-center: 332.0, Text: 0-0, Confidence: 97.22%\nBox 49, x-center: 607.0, Text: 0-0, Confidence: 97.16%\nBox 52, x-center: 377.0, Text: Qe2, Confidence: 96.19%\nBox 53, x-center: 578.5, Text: e5, Confidence: 99.46%\nBox 56, x-center: 344.5, Text: Rael, Confidence: 99.33%\nBox 57, x-center: 593.5, Text: Re8, Confidence: 99.46%\nBox 61, x-center: 333.5, Text: Qe4, Confidence: 94.86%\nBox 62, x-center: 585.5, Text: 96, Confidence: 96.09%\nBox 64, x-center: 655.5, Text: Bxf5, Confidence: 86.17%\nBox 66, x-center: 332.5, Text: Qe3, Confidence: 99.54%\nBox 69, x-center: 344.5, Text: 9xf5, Confidence: 88.79%\nBox 70, x-center: 614.5, Text: dxe5, Confidence: 98.72%\nBox 72, x-center: 383.0, Text: Nxe5, Confidence: 94.66%\nAnnotated image saved at: /kaggle/working/paddle_annotated/20241124-1012.jpg\nLabel CSV file saved at: /kaggle/working/paddle_labels/20241124-1012.jpg.csv\nProcessing 2024-11-24 10-11 1.jpeg...\n[2024/11/24 17:53:58] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n[2024/11/24 17:53:59] ppocr DEBUG: dt_boxes num : 143, elapsed : 0.3541889190673828\n[2024/11/24 17:54:05] ppocr DEBUG: rec_res num  : 143, elapsed : 5.995874404907227\nBox 1, x-center: 430.0, Text: OFFICIAL, Confidence: 99.27%\nBox 5, x-center: 1527.0, Text: 1978, Confidence: 56.72%\nBox 14, x-center: 1458.5, Text: FRANIC MARSHATL, Confidence: 86.05%\nBox 15, x-center: 482.5, Text: WHITE, Confidence: 99.78%\nBox 16, x-center: 792.5, Text: BLACK, Confidence: 99.80%\nBox 17, x-center: 1171.5, Text: WHITE, Confidence: 99.77%\nBox 18, x-center: 1481.5, Text: BLACK, Confidence: 99.82%\nBox 19, x-center: 1508.5, Text: Le6, Confidence: 91.57%\nBox 20, x-center: 491.0, Text: l y, Confidence: 54.27%\nBox 21, x-center: 802.5, Text: e5, Confidence: 92.89%\nBox 23, x-center: 1201.0, Text: g4+, Confidence: 86.13%\nBox 24, x-center: 512.5, Text: NF3, Confidence: 93.63%\nBox 25, x-center: 1204.0, Text: Rel+, Confidence: 86.49%\nBox 26, x-center: 797.5, Text: Nc 6, Confidence: 77.47%\nBox 28, x-center: 1507.0, Text: KdF, Confidence: 77.35%\nBox 31, x-center: 511.0, Text: Bb.5., Confidence: 73.32%\nBox 32, x-center: 791.0, Text: 06, Confidence: 93.57%\nBox 33, x-center: 1202.5, Text: Re?+, Confidence: 74.11%\nBox 34, x-center: 1511.0, Text: Kc6., Confidence: 80.62%\nBox 36, x-center: 803.5, Text: NFC, Confidence: 68.65%\nBox 37, x-center: 518.0, Text: Ba y, Confidence: 68.39%\nBox 39, x-center: 1208.5, Text: Rc Et, Confidence: 77.95%\nBox 40, x-center: 1502.5, Text: kb6, Confidence: 61.13%\nBox 42, x-center: 533.5, Text: 6-0, Confidence: 91.04%\nBox 43, x-center: 809.5, Text: Be 7, Confidence: 96.48%\nBox 44, x-center: 1204.0, Text: R6 8t, Confidence: 79.54%\nBox 45, x-center: 1526.5, Text: Kc6, Confidence: 66.13%\nBox 48, x-center: 506.5, Text: Re(, Confidence: 79.27%\nBox 49, x-center: 1530.5, Text: Kb6, Confidence: 85.60%\nBox 51, x-center: 819.0, Text: 65, Confidence: 78.90%\nBox 53, x-center: 1208.0, Text: Rc.8+, Confidence: 76.39%\nBox 54, x-center: 493.5, Text: B63, Confidence: 88.34%\nBox 55, x-center: 828.0, Text: 0-0, Confidence: 78.92%\nBox 56, x-center: 1514.0, Text: Kc.6, Confidence: 79.44%\nBox 58, x-center: 1207.0, Text: Rc 6+, Confidence: 93.55%\nBox 59, x-center: 1217.0, Text: d5+, Confidence: 97.36%\nBox 60, x-center: 1498.0, Text: Kb6, Confidence: 67.19%\nBox 61, x-center: 502.5, Text: c3, Confidence: 91.88%\nBox 62, x-center: 809.5, Text: d 5, Confidence: 82.91%\nBox 66, x-center: 518.0, Text: exd5, Confidence: 89.07%\nBox 67, x-center: 826.5, Text: Nxd5, Confidence: 80.86%\nBox 69, x-center: 1192.5, Text: Be3#, Confidence: 99.43%\nBox 71, x-center: 514.0, Text: Nxes, Confidence: 97.25%\nBox 72, x-center: 816.5, Text: Nx e5, Confidence: 91.74%\nBox 75, x-center: 510.5, Text: Qxes, Confidence: 91.14%\nBox 76, x-center: 832.0, Text: C6, Confidence: 76.62%\nBox 79, x-center: 523.5, Text: c y, Confidence: 59.41%\nBox 80, x-center: 822.0, Text: Bd6, Confidence: 82.16%\nBox 82, x-center: 511.0, Text: Rel, Confidence: 85.54%\nBox 83, x-center: 827.5, Text: Qh4, Confidence: 79.83%\nBox 86, x-center: 833.5, Text: Qh 3, Confidence: 91.44%\nBox 89, x-center: 523.5, Text: 93, Confidence: 98.59%\nBox 91, x-center: 482.5, Text: Rey, Confidence: 91.00%\nBox 93, x-center: 830.0, Text: 9.5, Confidence: 76.83%\nBox 95, x-center: 499.5, Text: QF 3, Confidence: 88.41%\nBox 98, x-center: 481.0, Text: Rel, Confidence: 83.96%\nBox 99, x-center: 838.0, Text: 9, Confidence: 60.42%\nBox 101, x-center: 522.0, Text: Bxd5t, Confidence: 80.43%\nBox 103, x-center: 805.0, Text: cxds, Confidence: 81.88%\nBox 105, x-center: 788.0, Text: Jeh., Confidence: 68.08%\nBox 106, x-center: 509.5, Text: Q+c5+, Confidence: 63.61%\nBox 109, x-center: 835.0, Text: f3, Confidence: 75.23%\nBox 110, x-center: 506.5, Text: Qxg8, Confidence: 71.86%\nBox 113, x-center: 501.0, Text: Qx E3, Confidence: 85.25%\nBox 114, x-center: 812.0, Text: RxE3, Confidence: 74.46%\nBox 118, x-center: 527.5, Text: Re8+, Confidence: 78.73%\nBox 119, x-center: 759.5, Text: LG, Confidence: 52.92%\nBox 120, x-center: 853.5, Text: 7, Confidence: 40.12%\nBox 125, x-center: 765.5, Text: V, Confidence: 60.82%\nBox 126, x-center: 829.0, Text: f5, Confidence: 89.21%\nBox 130, x-center: 523.5, Text: S5, Confidence: 69.76%\nBox 131, x-center: 516.0, Text: Nd2, Confidence: 92.75%\nBox 132, x-center: 813.5, Text: Rx f2, Confidence: 86.41%\nAnnotated image saved at: /kaggle/working/paddle_annotated/2024-11-24 10-11 1.jpeg\nLabel CSV file saved at: /kaggle/working/paddle_labels/2024-11-24 10-11 1.jpeg.csv\nProcessing 2024-11-24 10-12 1.jpeg...\n[2024/11/24 17:54:05] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n[2024/11/24 17:54:05] ppocr DEBUG: dt_boxes num : 120, elapsed : 0.22592806816101074\n[2024/11/24 17:54:10] ppocr DEBUG: rec_res num  : 120, elapsed : 5.4566521644592285\nBox 1, x-center: 360.0, Text: OFFICIAL, Confidence: 99.14%\nBox 6, x-center: 1485.5, Text: 1960, Confidence: 96.17%\nBox 15, x-center: 1357.0, Text: BOTVINNiK, Confidence: 84.01%\nBox 16, x-center: 414.5, Text: WHITE, Confidence: 99.76%\nBox 17, x-center: 721.0, Text: BLACK, Confidence: 99.73%\nBox 18, x-center: 1098.5, Text: WHITE, Confidence: 99.85%\nBox 19, x-center: 1410.0, Text: BLACK, Confidence: 99.83%\nBox 20, x-center: 423.0, Text: ey, Confidence: 97.11%\nBox 21, x-center: 753.0, Text: c6, Confidence: 74.68%\nBox 23, x-center: 433.0, Text: d y, Confidence: 92.34%\nBox 24, x-center: 747.0, Text: d5, Confidence: 93.43%\nBox 27, x-center: 427.5, Text: NC3, Confidence: 84.99%\nBox 29, x-center: 764.5, Text: dxe y, Confidence: 77.98%\nBox 31, x-center: 443.0, Text: Nxey, Confidence: 91.46%\nBox 32, x-center: 756.0, Text: NE6, Confidence: 83.39%\nBox 36, x-center: 447.5, Text: WxFot, Confidence: 53.64%\nBox 37, x-center: 753.0, Text: ekE6, Confidence: 74.81%\nBox 40, x-center: 413.0, Text: c 3, Confidence: 92.89%\nBox 41, x-center: 754.5, Text: Bd6, Confidence: 96.03%\nBox 43, x-center: 424.5, Text: Bd3, Confidence: 88.63%\nBox 44, x-center: 761.5, Text: 0-6, Confidence: 89.29%\nBox 47, x-center: 419.0, Text: Qc2, Confidence: 82.58%\nBox 49, x-center: 753.0, Text: ReEt, Confidence: 72.65%\nBox 51, x-center: 413.0, Text: Ne?, Confidence: 83.53%\nBox 52, x-center: 751.5, Text: h 5, Confidence: 93.45%\nBox 55, x-center: 424.5, Text: Be3, Confidence: 93.82%\nBox 57, x-center: 761.5, Text: No7, Confidence: 75.26%\nBox 60, x-center: 417.0, Text: 0-0-0, Confidence: 89.04%\nBox 62, x-center: 731.5, Text: Nf8, Confidence: 73.14%\nBox 64, x-center: 420.5, Text: h 3, Confidence: 99.66%\nBox 65, x-center: 751.5, Text: 6-5, Confidence: 96.82%\nBox 68, x-center: 403.0, Text: 9 4, Confidence: 62.86%\nBox 69, x-center: 757.0, Text: Bep, Confidence: 98.87%\nBox 71, x-center: 430.0, Text: gxh 5, Confidence: 74.07%\nBox 73, x-center: 758.5, Text: Bds, Confidence: 82.61%\nBox 75, x-center: 422.0, Text: Rh g1, Confidence: 80.00%\nBox 77, x-center: 757.5, Text: Ne6, Confidence: 78.84%\nBox 79, x-center: 413.0, Text: BhC, Confidence: 74.78%\nBox 81, x-center: 742.5, Text: Kh, Confidence: 98.19%\nBox 84, x-center: 447.5, Text: Bx gH, Confidence: 79.13%\nBox 85, x-center: 721.5, Text: 1x 9 t, Confidence: 44.53%\nBox 88, x-center: 399.0, Text: d, Confidence: 83.32%\nBox 89, x-center: 721.5, Text: BE8, Confidence: 83.01%\nBox 93, x-center: 411.5, Text: Q h 6t, Confidence: 74.74%\nBox 94, x-center: 748.5, Text: K58, Confidence: 83.12%\nBox 96, x-center: 387.5, Text: Rg.3, Confidence: 92.90%\nBox 97, x-center: 727.0, Text: Qd6, Confidence: 64.88%\nBox 102, x-center: 751.5, Text: 5, Confidence: 95.92%\nAnnotated image saved at: /kaggle/working/paddle_annotated/2024-11-24 10-12 1.jpeg\nLabel CSV file saved at: /kaggle/working/paddle_labels/2024-11-24 10-12 1.jpeg.csv\nProcessing 2024-11-24 10-13 2.jpeg...\n[2024/11/24 17:54:11] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n[2024/11/24 17:54:11] ppocr DEBUG: dt_boxes num : 177, elapsed : 0.420851469039917\n[2024/11/24 17:54:18] ppocr DEBUG: rec_res num  : 177, elapsed : 7.357981443405151\nBox 1, x-center: 382.5, Text: OFFICIAL, Confidence: 99.36%\nBox 2, x-center: 728.0, Text: 6, Confidence: 57.34%\nBox 15, x-center: 1590.0, Text: Karjaey, Confidence: 85.67%\nBox 16, x-center: 448.0, Text: WHITE, Confidence: 99.72%\nBox 17, x-center: 785.5, Text: BLACK, Confidence: 99.85%\nBox 18, x-center: 1197.0, Text: WHITE, Confidence: 99.81%\nBox 19, x-center: 1534.5, Text: BLACK, Confidence: 99.79%\nBox 20, x-center: 473.5, Text: e4, Confidence: 89.86%\nBox 21, x-center: 1222.5, Text: NG3, Confidence: 83.69%\nBox 22, x-center: 1539.0, Text: R57, Confidence: 93.74%\nBox 23, x-center: 766.5, Text: e5, Confidence: 86.29%\nBox 25, x-center: 1225.5, Text: ey, Confidence: 75.85%\nBox 27, x-center: 472.0, Text: Nf 3, Confidence: 90.28%\nBox 28, x-center: 818.5, Text: NC6, Confidence: 79.36%\nBox 30, x-center: 1510.5, Text: 0%, Confidence: 44.91%\nBox 31, x-center: 1593.5, Text: 0, Confidence: 74.44%\nBox 32, x-center: 470.5, Text: Bb5, Confidence: 79.09%\nBox 33, x-center: 849.0, Text: NC, Confidence: 69.26%\nBox 36, x-center: 1234.5, Text: f5, Confidence: 60.23%\nBox 37, x-center: 1571.0, Text: cxf5, Confidence: 79.99%\nBox 39, x-center: 476.5, Text: 0-6, Confidence: 86.98%\nBox 40, x-center: 834.0, Text: Nxey, Confidence: 90.79%\nBox 42, x-center: 1236.5, Text: Rxe 7, Confidence: 89.69%\nBox 43, x-center: 1512.0, Text: Rxe, Confidence: 97.00%\nBox 44, x-center: 1607.0, Text: 7, Confidence: 68.08%\nBox 46, x-center: 832.5, Text: N.dC, Confidence: 66.99%\nBox 48, x-center: 1537.5, Text: lg!, Confidence: 75.66%\nBox 49, x-center: 1604.0, Text: X, Confidence: 74.78%\nBox 50, x-center: 1253.0, Text: Rxds, Confidence: 74.91%\nBox 51, x-center: 460.0, Text: BxCC, Confidence: 80.71%\nBox 52, x-center: 1564.5, Text: Re6, Confidence: 90.46%\nBox 54, x-center: 818.5, Text: dx C6, Confidence: 81.80%\nBox 56, x-center: 1245.5, Text: NF3, Confidence: 76.61%\nBox 58, x-center: 484.0, Text: dces, Confidence: 86.58%\nBox 59, x-center: 824.5, Text: Nf 5, Confidence: 70.70%\nBox 60, x-center: 1608.5, Text: Ro6, Confidence: 69.41%\nBox 62, x-center: 1250.0, Text: Res, Confidence: 95.33%\nBox 63, x-center: 1588.5, Text: Rads, Confidence: 81.68%\nBox 65, x-center: 503.5, Text: Qxd8+, Confidence: 86.56%\nBox 66, x-center: 831.0, Text: Kxd8, Confidence: 77.16%\nBox 68, x-center: 1250.0, Text: Re2, Confidence: 97.79%\nBox 69, x-center: 499.0, Text: Nc 3, Confidence: 77.15%\nBox 71, x-center: 846.0, Text: le8, Confidence: 79.98%\nBox 73, x-center: 1257.5, Text: K52, Confidence: 59.61%\nBox 74, x-center: 1607.0, Text: 05, Confidence: 76.26%\nBox 75, x-center: 485.5, Text: h3, Confidence: 99.87%\nBox 76, x-center: 839.5, Text: 66, Confidence: 85.43%\nBox 77, x-center: 1262.5, Text: Kf2, Confidence: 63.69%\nBox 80, x-center: 1587.5, Text: a y, Confidence: 84.90%\nBox 82, x-center: 497.5, Text: RoI., Confidence: 67.50%\nBox 83, x-center: 831.0, Text: bb7, Confidence: 48.04%\nBox 85, x-center: 1271.0, Text: Ne5, Confidence: 92.16%\nBox 86, x-center: 1584.0, Text: Reb, Confidence: 88.54%\nBox 88, x-center: 827.5, Text: Ne7, Confidence: 88.70%\nBox 89, x-center: 1247.0, Text: Jd3, Confidence: 75.82%\nBox 90, x-center: 513.0, Text: 99, Confidence: 96.41%\nBox 92, x-center: 1579.5, Text: Rxe2+, Confidence: 90.31%\nBox 93, x-center: 842.5, Text: h 5, Confidence: 95.89%\nBox 95, x-center: 485.0, Text: Nhy, Confidence: 92.99%\nBox 97, x-center: 1233.5, Text: Kxe2, Confidence: 78.43%\nBox 98, x-center: 1595.0, Text: h4, Confidence: 98.27%\nBox 100, x-center: 841.5, Text: Bc 8, Confidence: 97.14%\nBox 102, x-center: 1218.0, Text: NF1, Confidence: 86.43%\nBox 103, x-center: 1570.5, Text: Ro[q, Confidence: 61.18%\nBox 104, x-center: 464.0, Text: 95, Confidence: 99.49%\nBox 105, x-center: 457.0, Text: th2, Confidence: 89.85%\nBox 106, x-center: 1227.0, Text: No2, Confidence: 67.90%\nBox 108, x-center: 817.0, Text: Be6, Confidence: 87.88%\nBox 110, x-center: 1587.5, Text: cy, Confidence: 86.55%\nBox 112, x-center: 467.5, Text: Ne2, Confidence: 80.65%\nBox 113, x-center: 803.5, Text: c5, Confidence: 95.44%\nBox 115, x-center: 1245.0, Text: Ne5, Confidence: 81.22%\nBox 116, x-center: 1576.5, Text: Rd 5, Confidence: 86.08%\nBox 118, x-center: 467.5, Text: Be 3, Confidence: 96.44%\nBox 119, x-center: 811.0, Text: Bc6, Confidence: 86.93%\nBox 121, x-center: 1260.5, Text: Ndf3, Confidence: 91.28%\nBox 122, x-center: 1569.5, Text: Rb5, Confidence: 76.80%\nBox 123, x-center: 478.0, Text: Ag3, Confidence: 48.29%\nBox 124, x-center: 826.0, Text: 96, Confidence: 98.72%\nBox 126, x-center: 1212.0, Text: 63, Confidence: 99.11%\nBox 127, x-center: 1570.5, Text: cx63, Confidence: 66.63%\nBox 130, x-center: 441.5, Text: e6, Confidence: 60.30%\nBox 131, x-center: 831.0, Text: fxe6, Confidence: 78.32%\nBox 132, x-center: 1192.0, Text: Cx63, Confidence: 75.46%\nBox 134, x-center: 1548.0, Text: ax63, Confidence: 89.39%\nBox 135, x-center: 455.5, Text: B Fy, Confidence: 73.05%\nBox 137, x-center: 817.0, Text: Rh 7, Confidence: 86.71%\nBox 139, x-center: 1205.5, Text: ax b3, Confidence: 72.93%\nBox 140, x-center: 1554.0, Text: C5, Confidence: 76.23%\nBox 142, x-center: 484.0, Text: Rd2, Confidence: 99.41%\nBox 143, x-center: 814.0, Text: RFF, Confidence: 84.46%\nBox 144, x-center: 1228.5, Text: Ncy, Confidence: 85.74%\nBox 145, x-center: 1555.5, Text: Rx63, Confidence: 86.72%\nBox 147, x-center: 461.5, Text: Be 5, Confidence: 92.16%\nBox 149, x-center: 821.5, Text: Bet, Confidence: 56.13%\nBox 151, x-center: 1235.0, Text: Nfes, Confidence: 85.86%\nBox 152, x-center: 1569.0, Text: Rc3, Confidence: 96.94%\nBox 153, x-center: 481.0, Text: Bxg7, Confidence: 64.71%\nBox 155, x-center: 809.5, Text: Rxg7, Confidence: 76.10%\nBox 157, x-center: 1233.0, Text: Rd, Confidence: 67.89%\nBox 158, x-center: 1558.5, Text: Rxh3, Confidence: 99.34%\nBox 159, x-center: 458.5, Text: Re7., Confidence: 76.43%\nBox 160, x-center: 802.0, Text: Bds, Confidence: 76.09%\nBox 161, x-center: 1224.0, Text: Nd3, Confidence: 96.56%\nBox 164, x-center: 1575.0, Text: Rh Qf, Confidence: 75.23%\nBox 165, x-center: 473.5, Text: Ne?, Confidence: 82.37%\nBox 166, x-center: 811.0, Text: RF7, Confidence: 88.79%\nBox 168, x-center: 1231.5, Text: Kc 3, Confidence: 80.26%\nBox 170, x-center: 1561.5, Text: 65, Confidence: 93.43%\nAnnotated image saved at: /kaggle/working/paddle_annotated/2024-11-24 10-13 2.jpeg\nLabel CSV file saved at: /kaggle/working/paddle_labels/2024-11-24 10-13 2.jpeg.csv\nProcessing 2024-11-24 10-10 1.jpeg...\n[2024/11/24 17:54:18] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n[2024/11/24 17:54:19] ppocr DEBUG: dt_boxes num : 178, elapsed : 0.2476975917816162\n[2024/11/24 17:54:26] ppocr DEBUG: rec_res num  : 178, elapsed : 7.39101767539978\nBox 1, x-center: 283.0, Text: OFFICIAL, Confidence: 99.47%\nBox 5, x-center: 1276.0, Text: 7999, Confidence: 79.30%\nBox 13, x-center: 561.0, Text: GARRY IASPARDV, Confidence: 85.06%\nBox 15, x-center: 342.0, Text: WHITE, Confidence: 99.81%\nBox 16, x-center: 618.0, Text: BLACK, Confidence: 99.76%\nBox 17, x-center: 956.0, Text: WHITE, Confidence: 99.78%\nBox 18, x-center: 1234.0, Text: BLACK, Confidence: 99.82%\nBox 19, x-center: 325.0, Text: ey, Confidence: 93.76%\nBox 20, x-center: 637.0, Text: d6, Confidence: 84.19%\nBox 22, x-center: 978.0, Text: Rx04, Confidence: 71.16%\nBox 23, x-center: 1243.0, Text: cxcly., Confidence: 72.96%\nBox 24, x-center: 329.5, Text: dy, Confidence: 83.53%\nBox 27, x-center: 987.5, Text: Qxd4+, Confidence: 90.10%\nBox 29, x-center: 627.0, Text: Nf6, Confidence: 87.14%\nBox 30, x-center: 1253.0, Text: Q66, Confidence: 99.23%\nBox 31, x-center: 352.5, Text: N.C 3, Confidence: 79.34%\nBox 32, x-center: 644.0, Text: 96, Confidence: 98.41%\nBox 34, x-center: 1238.0, Text: 367, Confidence: 89.54%\nBox 36, x-center: 972.5, Text: Re 7+, Confidence: 81.65%\nBox 37, x-center: 635.5, Text: Bg7, Confidence: 68.00%\nBox 38, x-center: 355.0, Text: Be3, Confidence: 86.58%\nBox 40, x-center: 959.0, Text: Q57, Confidence: 89.13%\nBox 41, x-center: 1252.0, Text: R6 8, Confidence: 89.65%\nBox 43, x-center: 367.5, Text: Qd2, Confidence: 85.21%\nBox 45, x-center: 963.5, Text: 392, Confidence: 82.07%\nBox 46, x-center: 1263.5, Text: Qqlt, Confidence: 77.77%\nBox 48, x-center: 645.5, Text: C6, Confidence: 61.28%\nBox 49, x-center: 354.5, Text: A3, Confidence: 71.04%\nBox 50, x-center: 962.5, Text: Ka2, Confidence: 66.92%\nBox 51, x-center: 1258.5, Text: Qb6, Confidence: 79.22%\nBox 52, x-center: 649.0, Text: 65, Confidence: 94.48%\nBox 55, x-center: 370.5, Text: Nge?, Confidence: 89.40%\nBox 56, x-center: 649.0, Text: Nbd 7, Confidence: 85.83%\nBox 57, x-center: 1253.5, Text: 6d6, Confidence: 46.45%\nBox 60, x-center: 962.5, Text: fy, Confidence: 71.46%\nBox 61, x-center: 351.0, Text: Bh6, Confidence: 70.26%\nBox 62, x-center: 637.5, Text: Bxh6, Confidence: 92.48%\nBox 65, x-center: 1259.5, Text: Rxb7, Confidence: 80.97%\nBox 66, x-center: 991.5, Text: Rx67+, Confidence: 91.58%\nBox 67, x-center: 618.0, Text: B67, Confidence: 88.84%\nBox 69, x-center: 1273.0, Text: Ka 8, Confidence: 78.32%\nBox 71, x-center: 363.5, Text: Qxh6, Confidence: 86.57%\nBox 72, x-center: 992.5, Text: NC6t, Confidence: 76.27%\nBox 73, x-center: 984.0, Text: Qx h 8, Confidence: 95.99%\nBox 74, x-center: 1274.5, Text: N68, Confidence: 72.20%\nBox 76, x-center: 362.5, Text: 03, Confidence: 76.20%\nBox 77, x-center: 622.5, Text: e 5, Confidence: 95.03%\nBox 79, x-center: 962.5, Text: Qc 8, Confidence: 91.08%\nBox 80, x-center: 1262.0, Text: Qc7, Confidence: 95.30%\nBox 82, x-center: 376.5, Text: 0-0-0, Confidence: 89.99%\nBox 83, x-center: 637.0, Text: Qe7, Confidence: 77.92%\nBox 86, x-center: 373.5, Text: Kb 1, Confidence: 93.35%\nBox 88, x-center: 977.5, Text: Qe6, Confidence: 97.37%\nBox 89, x-center: 1259.5, Text: Qd7, Confidence: 69.17%\nBox 90, x-center: 658.0, Text: 06, Confidence: 81.76%\nBox 92, x-center: 1269.5, Text: Rc F, Confidence: 90.05%\nBox 94, x-center: 375.0, Text: Nc L, Confidence: 67.99%\nBox 95, x-center: 655.5, Text: 0-0-6, Confidence: 88.00%\nBox 96, x-center: 978.0, Text: Qes, Confidence: 79.64%\nBox 97, x-center: 1278.5, Text: K6 7, Confidence: 88.45%\nBox 99, x-center: 377.5, Text: N63, Confidence: 87.99%\nBox 100, x-center: 664.0, Text: ex d!4, Confidence: 83.97%\nBox 102, x-center: 990.0, Text: Nd y, Confidence: 62.73%\nBox 104, x-center: 407.5, Text: Rxdy, Confidence: 87.20%\nBox 105, x-center: 677.0, Text: C5, Confidence: 70.22%\nBox 107, x-center: 992.5, Text: d6t, Confidence: 96.01%\nBox 108, x-center: 1273.5, Text: Rc6, Confidence: 92.43%\nBox 109, x-center: 1271.0, Text: Qd8, Confidence: 83.89%\nBox 111, x-center: 390.0, Text: Rd/, Confidence: 79.72%\nBox 112, x-center: 659.5, Text: Nb6, Confidence: 91.57%\nBox 114, x-center: 975.5, Text: N63, Confidence: 82.52%\nBox 115, x-center: 653.0, Text: K69, Confidence: 60.75%\nBox 116, x-center: 1269.5, Text: Kb.6, Confidence: 61.50%\nBox 119, x-center: 993.0, Text: Na 5t, Confidence: 85.53%\nBox 120, x-center: 364.0, Text: 93, Confidence: 98.36%\nBox 121, x-center: 385.0, Text: NQ5, Confidence: 53.27%\nBox 122, x-center: 652.0, Text: Ba%, Confidence: 87.50%\nBox 125, x-center: 989.0, Text: Qd4+, Confidence: 75.70%\nBox 126, x-center: 1284.5, Text: Nc 5, Confidence: 74.63%\nBox 127, x-center: 382.5, Text: Bh 3, Confidence: 98.74%\nBox 128, x-center: 669.5, Text: 05, Confidence: 74.43%\nBox 130, x-center: 985.0, Text: Qe5, Confidence: 93.81%\nBox 131, x-center: 1278.5, Text: N67, Confidence: 88.55%\nBox 132, x-center: 397.5, Text: QE4+, Confidence: 84.38%\nBox 133, x-center: 674.5, Text: Ra.7, Confidence: 83.06%\nBox 135, x-center: 986.5, Text: Bx.c6, Confidence: 70.27%\nBox 137, x-center: 1259.5, Text: Qc8, Confidence: 95.74%\nBox 138, x-center: 390.0, Text: Rhel, Confidence: 94.07%\nBox 140, x-center: 670.5, Text: d 4, Confidence: 91.49%\nBox 142, x-center: 986.0, Text: Bxb7, Confidence: 57.56%\nBox 143, x-center: 1264.5, Text: Qx67, Confidence: 94.55%\nBox 144, x-center: 385.0, Text: N d5, Confidence: 85.91%\nBox 145, x-center: 663.0, Text: N fxd3, Confidence: 70.38%\nBox 146, x-center: 999.0, Text: Qd4+, Confidence: 87.54%\nBox 147, x-center: 1252.0, Text: k66, Confidence: 75.22%\nBox 151, x-center: 1274.5, Text: Qc F, Confidence: 85.21%\nBox 152, x-center: 375.0, Text: ex d5, Confidence: 96.10%\nBox 153, x-center: 667.0, Text: Q d 6, Confidence: 93.67%\nBox 155, x-center: 962.5, Text: d+, Confidence: 70.40%\nBox 157, x-center: 373.5, Text: R e 5, Confidence: 96.48%\nBox 159, x-center: 1003.0, Text: Nc 4 f, Confidence: 89.49%\nBox 160, x-center: 1276.0, Text: K6 5, Confidence: 88.96%\nBox 161, x-center: 679.0, Text: 5, Confidence: 94.36%\nBox 162, x-center: 369.0, Text: b4, Confidence: 87.10%\nBox 163, x-center: 1277.0, Text: Qxc4+, Confidence: 90.66%\nBox 164, x-center: 668.0, Text: Na4, Confidence: 75.95%\nBox 167, x-center: 996.5, Text: Q d6, Confidence: 93.59%\nBox 172, x-center: 268.0, Text: SIGNATURE, Confidence: 99.66%\nAnnotated image saved at: /kaggle/working/paddle_annotated/2024-11-24 10-10 1.jpeg\nLabel CSV file saved at: /kaggle/working/paddle_labels/2024-11-24 10-10 1.jpeg.csv\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"zip annotated files for download","metadata":{}},{"cell_type":"code","source":"#zip annotated files and labels for download\n!zip -r /kaggle/working/paddle_labels/paddle_labels.zip /kaggle/working/paddle_labels\n!zip -r /kaggle/working/paddle_annotated/paddle_annotated.zip /kaggle/working/paddle_annotated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:55:44.204557Z","iopub.execute_input":"2024-11-24T17:55:44.205426Z","iopub.status.idle":"2024-11-24T17:55:47.161850Z","shell.execute_reply.started":"2024-11-24T17:55:44.205384Z","shell.execute_reply":"2024-11-24T17:55:47.160603Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/paddle_labels/ (stored 0%)\nupdating: kaggle/working/paddle_labels/2024-11-23 23-04.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 23-14.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 23-00.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 22-58.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 23-13.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 15-37 1.jpeg.csv (deflated 64%)\nupdating: kaggle/working/paddle_labels/2024-11-23 15-34.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 22-59.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/034_0.png.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/{image_name}.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 15-37.jpeg.csv (deflated 65%)\nupdating: kaggle/working/paddle_labels/2024-11-23 15-36.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 15-35.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 23-00 1.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 15-36 1.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/labels.csv (deflated 77%)\nupdating: kaggle/working/paddle_labels/2024-11-23 23-05.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 22-59 1.jpeg.csv (deflated 66%)\nupdating: kaggle/working/paddle_labels/2024-11-23 22-57.jpeg.csv (deflated 65%)\nupdating: kaggle/working/paddle_labels/2024-11-24 10-13.jpeg.csv (deflated 63%)\nupdating: kaggle/working/paddle_labels/2024-11-24 10-13 1.jpeg.csv (deflated 62%)\nupdating: kaggle/working/paddle_labels/2024-11-24 10-12.jpeg.csv (deflated 64%)\nupdating: kaggle/working/paddle_labels/2024-11-24 10-09.jpeg.csv (deflated 64%)\nupdating: kaggle/working/paddle_labels/20241124-1013.jpg.csv (deflated 63%)\nupdating: kaggle/working/paddle_labels/2024-11-24 10-11.jpeg.csv (deflated 64%)\n  adding: kaggle/working/paddle_labels/2024-11-24 10-11 1.jpeg.csv (deflated 63%)\n  adding: kaggle/working/paddle_labels/2024-11-24 10-13 2.jpeg.csv (deflated 64%)\n  adding: kaggle/working/paddle_labels/2024-11-24 10-10 1.jpeg.csv (deflated 64%)\n  adding: kaggle/working/paddle_labels/20241124-1012.jpg.csv (deflated 64%)\n  adding: kaggle/working/paddle_labels/2024-11-24 10-12 1.jpeg.csv (deflated 63%)\nupdating: kaggle/working/paddle_annotated/ (stored 0%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 15-35.jpeg (deflated 7%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 15-37 1.jpeg (deflated 16%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 15-34.jpeg (deflated 15%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 22-59.jpeg (deflated 22%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 15-37.jpeg (deflated 16%)\nupdating: kaggle/working/paddle_annotated/034_0.png (deflated 0%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 22-58.jpeg (deflated 24%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 15-36.jpeg (deflated 13%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 23-00.jpeg (deflated 20%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 23-04.jpeg (deflated 20%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 23-14.jpeg (deflated 24%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 22-59 1.jpeg (deflated 20%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 15-36 1.jpeg (deflated 11%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 23-05.jpeg (deflated 22%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 23-13.jpeg (deflated 22%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 22-57.jpeg (deflated 21%)\nupdating: kaggle/working/paddle_annotated/2024-11-23 23-00 1.jpeg (deflated 20%)\nupdating: kaggle/working/paddle_annotated/2024-11-24 10-13.jpeg (deflated 16%)\nupdating: kaggle/working/paddle_annotated/20241124-1013.jpg (deflated 2%)\nupdating: kaggle/working/paddle_annotated/2024-11-24 10-11.jpeg (deflated 16%)\nupdating: kaggle/working/paddle_annotated/2024-11-24 10-12.jpeg (deflated 16%)\nupdating: kaggle/working/paddle_annotated/2024-11-24 10-13 1.jpeg (deflated 16%)\nupdating: kaggle/working/paddle_annotated/2024-11-24 10-09.jpeg (deflated 16%)\n  adding: kaggle/working/paddle_annotated/2024-11-24 10-12 1.jpeg (deflated 16%)\n  adding: kaggle/working/paddle_annotated/2024-11-24 10-13 2.jpeg (deflated 18%)\n  adding: kaggle/working/paddle_annotated/2024-11-24 10-10 1.jpeg (deflated 15%)\n  adding: kaggle/working/paddle_annotated/20241124-1012.jpg (deflated 2%)\n  adding: kaggle/working/paddle_annotated/2024-11-24 10-11 1.jpeg (deflated 17%)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"**Order moves using columns if there are no white and black columns**","metadata":{}},{"cell_type":"code","source":"#function organize_boxes_by_rows_and_columns\nfrom PIL import Image\n\ndef organize_boxes_by_rows_and_columns(res, image):\n    \"\"\"\n    Organize OCR-detected boxes into rows and calculate column widths dynamically using the image dimensions.\n    Boxes in each row are sorted based on their x_center.\n\n    Args:\n        res (list): OCR result containing boxes, text, and confidence scores.\n        image (Image): A Pillow Image object for calculating the image dimensions.\n\n    Returns:\n        dict: A dictionary with organized rows and calculated column widths.\n    \"\"\"\n    if not res:\n        print(\"No OCR results to process.\")\n        return {\"rows\": [], \"column_widths\": []}\n\n    # Get the image dimensions\n    image_width, image_height = image.size\n\n    # Process OCR results and calculate x_center and y_center for each box\n    boxes = []\n    for idx, item in enumerate(res):\n        # OCR result structure: [(box_coordinates, (\"text\", confidence))]\n        box_coordinates = item[0]\n        text, confidence = item[1]\n\n        x_left, y_top = box_coordinates[0]\n        x_right, y_bottom = box_coordinates[2]\n        x_center = (x_left + x_right) / 2\n        y_center = (y_top + y_bottom) / 2\n\n        boxes.append({\n            \"box_number\": idx + 1,\n            \"x_center\": x_center,\n            \"y_center\": y_center,\n            \"text\": text,\n            \"confidence\": confidence,\n            \"coordinates\": [x_left, y_top, x_right, y_bottom]\n        })\n\n    # Sort boxes by y_center\n    boxes = sorted(boxes, key=lambda b: b['y_center'])\n\n    # Group boxes into rows\n    rows = []\n    current_row = []\n\n    for box in boxes:\n        if not current_row:\n            # Start a new row with the first box\n            current_row.append(box)\n        else:\n            # Determine the y_min and y_max of the current row\n            y_min = min(b['coordinates'][1] for b in current_row)\n            y_max = max(b['coordinates'][3] for b in current_row)\n\n            if y_min <= box['y_center'] <= y_max:\n                # Add the box to the current row if it fits\n                current_row.append(box)\n            else:\n                # Finalize the current row and start a new one\n                current_row = sorted(current_row, key=lambda b: b['x_center'])\n                rows.append(current_row)\n                current_row = [box]\n\n    if current_row:  # Add the last row\n        current_row = sorted(current_row, key=lambda b: b['x_center'])\n        rows.append(current_row)\n\n    # Adjust x_right for the first row\n    first_row = rows[0] if rows else []\n    for box in first_row:\n        box['coordinates'][2] += 10  # Add 10 pixels to x_right\n\n    # Calculate column widths based on the first row\n    column_widths = []\n    if first_row:\n        for idx, box in enumerate(first_row):\n            x_max = box['coordinates'][2]  # x_max of the current box\n            if idx == 0:\n                # First column starts at 0\n                column_widths.append((0, x_max + 10))\n            elif idx == len(first_row) - 1:\n                # Last column ends at image width\n                column_widths.append((column_widths[-1][1], image_width))\n            else:\n                # Intermediate columns\n                prev_x_max = first_row[idx - 1]['coordinates'][2]\n                column_widths.append((prev_x_max + 10, x_max + 5))\n\n    return {\"rows\": rows, \"column_widths\": column_widths}\n\ndef exclude_columns(rows, column_widths, excluded_columns):\n    \"\"\"\n    Exclude boxes that fall into specific columns based on column widths.\n\n    Args:\n        rows (list): List of rows, where each row contains boxes.\n        column_widths (list): List of column boundaries as tuples (start, end).\n        excluded_columns (list): List of column indices to exclude (1-based).\n\n    Returns:\n        list: A flat list of all boxes except those in the excluded columns.\n    \"\"\"\n    excluded_boxes = []\n\n    # Convert 1-based column indices to 0-based for indexing\n    excluded_columns = [col - 1 for col in excluded_columns]\n\n    for row in rows:\n        for box in row:\n            # Determine the column of the current box based on its x_center\n            for col_index, (start, end) in enumerate(column_widths):\n                if start <= box[\"x_center\"] < end:\n                    if col_index not in excluded_columns:\n                        excluded_boxes.append(box)\n                    break\n\n    return excluded_boxes","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#run organize_boxes_by_rows_and_columns(res, image)\nresult = organize_boxes_by_rows_and_columns(res, image)\nprint (len(result[\"rows\"]))\nprint (\"first row:\",result[\"rows\"][0])\nprint (\"first row count:\",len(result[\"rows\"][0]))\nprint(\"columns\",result[\"column_widths\"])\nprint(\"columns count:\",len(result[\"column_widths\"]))\n\n# Assume `rows` and `column_widths` are returned by the organize_boxes_by_rows_and_columns function.\nexcluded_columns = [1, 4]  # Exclude columns 1 and 4\npreserved_boxes = exclude_columns(result[\"rows\"], result[\"column_widths\"], excluded_columns)\n\n# Print remaining boxes\nprint(preserved_boxes[0])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#order chess moves for preserved_boxes variable\n# Step 0: Calculate y_center for each box\nfor box in preserved_boxes:\n    # Check if 'y_center' already exists; calculate only if it's missing\n    if \"y_center\" not in box:\n        # Calculate the y_center as the midpoint of the y-coordinates\n        box[\"y_center\"] = (box[\"coordinates\"][0][1] + box[\"coordinates\"][2][1]) / 2\n# Step 1: Group boxes into rows by y_center\nrows = []  # List of rows\nfor box in preserved_boxes:\n    added_to_row = False \nfor row in rows:\n    # Check if the row already has 'row_y_min' and 'row_y_max'\n    if \"row_y_min\" not in row[0] or \"row_y_max\" not in row[0]:\n        # Calculate row_y_min and row_y_max only if missing\n        row_y_min = min(b[\"coordinates\"][0][1] for b in row)\n        row_y_max = max(b[\"coordinates\"][2][1] for b in row)\n\n        # Assign to each box in the row\n        for b in row:\n            b[\"row_y_min\"] = row_y_min\n            b[\"row_y_max\"] = row_y_max\n        if row_y_min <= box[\"y_center\"] <= row_y_max:\n            row.append(box)\n            added_to_row = True\n            break\n    if not added_to_row:\n        rows.append([box])  # Start a new row\n\n# Step 2: Sort each row by x_center\nfor row in rows:\n    row.sort(key=lambda b: b[\"x_center\"])\n\n# Step 3: Split rows into two lists\nfirst_list = []  # Rows with up to 2 elements\nsecond_list = []  # Overflow rows with more than 2 elements\n\nfor row in rows:\n    if len(row) <= 2:\n        first_list.append(row)\n    else:\n        # Add the first two elements to the first list\n        first_list.append(row[:2])\n        # Add the remaining elements as a new row in the second list\n        overflow = row[2:]\n        # Exclude rows with \"WHITE\" or \"BLACK\" in the text from the second list\n        filtered_overflow = [box for box in overflow if box[\"text\"].lower() not in [\"white\", \"black\"]]\n        if filtered_overflow:\n            second_list.append(filtered_overflow)\n\n# Step 4: Print the results\nprint(\"First List (Rows with up to 2 elements):\")\nfor i, row in enumerate(first_list):\n    print(f\"Row {i + 1}:\")\n    for box in row:\n        y_coords = [box[\"coordinates\"][0][1], box[\"coordinates\"][2][1]]\n        print(f\"  Box {box['box_number']}, x_center: {box['x_center']:.2f}, y_center: {box['y_center']:.2f}, \"\n              f\"y_coords: {y_coords}, Text: {box['text']}, Confidence: {box['confidence']:.2f}%\")\n\nprint(\"\\nSecond List (Overflow Rows):\")\nfor i, row in enumerate(second_list):\n    print(f\"Row {i + 1}:\")\n    for box in row:\n        y_coords = [box[\"coordinates\"][0][1], box[\"coordinates\"][2][1]]\n        print(f\"  Box {box['box_number']}, x_center: {box['x_center']:.2f}, y_center: {box['y_center']:.2f}, \"\n              f\"y_coords: {y_coords}, Text: {box['text']}, Confidence: {box['confidence']:.2f}%\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Calculate Accuracy based on Original text and label. 100% means that labels were not cleaned up**\nOCR accuracy before additional model training (paddleocr):\n2024-11-23 23-00.jpeg.csv      | Total_Rows: 130  | Matched_Rows: 103  | Total_Accuracy:  79.23 | Chess_Moves: 82   | Chess_Move_Accuracy: 62  | Move_Accuracy:  75.61\r\n2024-11-23 22-59.jpeg.csv      | Total_Rows: 104  | Matched_Rows: 95   | Total_Accuracy:  91.35 | Chess_Moves: 64   | Chess_Move_Accuracy: 60  | Move_Accuracy:  93.75\r\n2024-11-23 15-36.jpeg.csv      | Total_Rows: 119  | Matched_Rows: 105  | Total_Accuracy:  88.24 | Chess_Moves: 74   | Chess_Move_Accuracy: 64  | Move_Accuracy:  86.49\r\n2024-11-23 23-13.jpeg.csv      | Total_Rows: 130  | Matched_Rows: 117  | Total_Accuracy:  90.00 | Chess_Moves: 82   | Chess_Move_Accuracy: 72  | Move_Accuracy:  87.80\r\n2024-11-23 23-04.jpeg.csv      | Total_Rows: 157  | Matched_Rows: 128  | Total_Accuracy:  81.53 | Chess_Moves: 100  | Chess_Move_Accuracy: 79  | Move_Accuracy:  79.00\r\n2024-11-23 15-35.jpeg.csv      | Total_Rows: 97   | Matched_Rows: 89   | Total_Accuracy:  91.75 | Chess_Moves: 60   | Chess_Move_Accuracy: 54  | Move_Accuracy:  90.00\r\n2024-11-23 23-05.jpeg.csv      | Total_Rows: 112  | Matched_Rows: 99   | Total_Accuracy:  88.39 | Chess_Moves: 70   | Chess_Move_Accuracy: 62  | Move_Accuracy:  88.57\r\n2024-11-23 15-36 1.jpeg.csv    | Total_Rows: 166  | Matched_Rows: 135  | Total_Accuracy:  81.33 | Chess_Moves: 106  | Chess_Move_Accuracy: 86  | Move_Accuracy:  81.13\r\n2024-11-23 15-34.jpeg.csv      | Total_Rows: 98   | Matched_Rows: 93   | Total_Accuracy:  94.90 | Chess_Moves: 60   | Chess_Move_Accuracy: 57  | Move_Accuracy:  95.00\r\n2024-11-23 22-58.jpeg.csv      | Total_Rows: 110  | Matched_Rows: 99   | Total_Accuracy:  90.00 | Chess_Moves: 68   | Chess_Move_Accuracy: 60  | Move_Accuracy:  88.24\r\n2024-11-23 23-00 1.jpeg.csv    | Total_Rows: 108  | Matched_Rows: 84   | Total_Accuracy:  77.78 | Chess_Moves: 67   | Chess_Move_Accuracy: 53  | Move_Accuracy:  79.10\r\n2024-11-23 23-14.jpeg.csv      | Total_Rows: 108  | Matched_Rows: 95   | Total_Accuracy:  87.96 | Chess_Moves: 67   | Chess_Move_Accuracy: 59  | Move_Accuracy:  88.06\r\n2024-11-23 15-37 1.jpeg.csv    | Total_Rows: 159  | Matched_Rows: 114  | Total_Accuracy:  71.70 | Chess_Moves: 101  | Chess_Move_Accuracy: 72  | Move_Accuracy:  71.29\r\n2024-11-23 22-59 1.jpeg.csv    | Total_Rows: 107  | Matched_Rows: 92   | Total_Accuracy:  85.98 | Chess_Moves: 66   | Chess_Move_Accuracy: 58  | Move_Accuracy:  87.88\r\n2024-11-23 22-57.jpeg.csv      | Total_Rows: 160  | Matched_Rows: 132  | Total_Accuracy:  82.50 | Chess_Moves: 102  | Chess_Move_Accuracy: 80  | Move_Accuracy:  78.43\r\n\r\nAccuracy results saved to: /kaggle/working/accuracy/accuracy_revision_4.csvy_revision_1.\nc2024-11-24 10-10 1.jpeg.csv    | Total_Rows: 108  | Matched_Rows: 37   | Total_Accuracy:  34.26 | Chess_Moves: 108  | Chess_Move_Accuracy: 37  | Move_Accuracy:  34.26\r\n2024-11-24 10-11 1.jpeg.csv    | Total_Rows: 73   | Matched_Rows: 24   | Total_Accuracy:  32.88 | Chess_Moves: 50   | Chess_Move_Accuracy: 21  | Move_Accuracy:  42.00\r\n2024-11-24 10-12 1.jpeg.csv    | Total_Rows: 48   | Matched_Rows: 18   | Total_Accuracy:  37.50 | Chess_Moves: 37   | Chess_Move_Accuracy: 14  | Move_Accuracy:  37.84\r\n2024-11-24 10-13 2.jpeg.csv    | Total_Rows: 109  | Matched_Rows: 39   | Total_Accuracy:  35.78 | Chess_Moves: 77   | Chess_Move_Accuracy: 31  | Move_Accuracy:  40.26\r\n20241124-1012.jpg.csv          | Total_Rows: 38   | Matched_Rows: 32   | Total_Accuracy:  84.21 | Chess_Moves: 27   | Chess_Move_Accuracy: 21  | Move_Accuracy:  77.78\r\n\r\nAccuracy results saved to: /kaggle/working/accuracy/accuracy_revision_5.csv: 27         | Move_Accuracy:  77.78curacy:  80.90","metadata":{}},{"cell_type":"code","source":"#calculate accuracy. Requires human check for label column in csv files and update of the dataset\nimport pandas as pd\nimport os\n\n# Directory path containing the CSV files\ndirectory_path = '/kaggle/input/danny-chess-2/testing'  # Replace with your folder path\noutput_directory = '/kaggle/working/accuracy'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Initialize variables for combined metrics\ncombined_total_rows = 0\ncombined_matches = 0\ncombined_preserved_rows = 0\ncombined_preserved_matches = 0\n\n# Create a list to store file-specific results for CSV output\ncsv_output = []\n\n# Iterate through all CSV files in the directory\nfor file_name in os.listdir(directory_path):\n    if file_name.endswith('.csv'):  # Process only CSV files\n        file_path = os.path.join(directory_path, file_name)\n        \n        try:\n            data = pd.read_csv(file_path)\n        except Exception as e:\n            print(f\"Error reading {file_name}: {e}\")\n            continue\n\n        # Check for required column\n        if 'Box Number' not in data.columns:\n            print(f\"Skipping {file_name}: 'Box Number' column not found.\")\n            continue\n\n        # Calculate Total Accuracy for the current file\n        data['Match'] = data['Original Text'] == data['Label']\n        total_rows = len(data)\n        matches = data['Match'].sum()\n        total_accuracy = (matches / total_rows) * 100 if total_rows > 0 else 0\n\n        # Filter rows for preserved boxes\n        preserved_box_numbers = [box[\"box_number\"] for box in preserved_boxes]  # Replace preserved_boxes dynamically\n        preserved_data = data[data['Box Number'].isin(preserved_box_numbers)]\n        preserved_matches = (preserved_data['Original Text'] == preserved_data['Label']).sum()\n        preserved_rows = len(preserved_data)\n        move_accuracy = (preserved_matches / preserved_rows) * 100 if preserved_rows > 0 else 0\n\n        # Print results for the current file in table format\n        print(f\"{file_name:30} | Total_Rows: {total_rows:<4} | Matched_Rows: {matches:<4} | Total_Accuracy: {total_accuracy:6.2f} | Chess_Moves: {preserved_rows:<4} | Chess_Move_Accuracy: {preserved_matches:<4}| Move_Accuracy: {move_accuracy:6.2f}\")\n\n        # Add results to the CSV output list\n        csv_output.append({\n            \"Filename\": file_name,\n            \"Total_Rows\": total_rows,\n            \"Matched_Rows\": matches,\n            \"Total_Accuracy\": total_accuracy,\n            \"Chess_Moves\": preserved_rows,\n            \"Chess_Move_Accuracy\": preserved_matches,\n            \"Move_Accuracy\": move_accuracy\n        })\n\n\n# Find the next available revision number for the output file\nrevision = 1\nwhile os.path.exists(os.path.join(output_directory, f\"accuracy_revision_{revision}.csv\")):\n    revision += 1\noutput_file_path = os.path.join(output_directory, f\"accuracy_revision_{revision}.csv\")\n\n# Save results to a CSV file\ncsv_df = pd.DataFrame(csv_output)\ncsv_df.to_csv(output_file_path, index=False)\nprint(f\"\\nAccuracy results saved to: {output_file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T20:03:43.742559Z","iopub.execute_input":"2024-11-24T20:03:43.743009Z","iopub.status.idle":"2024-11-24T20:03:43.786264Z","shell.execute_reply.started":"2024-11-24T20:03:43.742968Z","shell.execute_reply":"2024-11-24T20:03:43.785104Z"}},"outputs":[{"name":"stdout","text":"2024-11-24 10-10 1.jpeg.csv    | Total_Rows: 108  | Matched_Rows: 37   | Total_Accuracy:  34.26 | Chess_Moves: 108  | Chess_Move_Accuracy: 37  | Move_Accuracy:  34.26\n2024-11-24 10-11 1.jpeg.csv    | Total_Rows: 73   | Matched_Rows: 24   | Total_Accuracy:  32.88 | Chess_Moves: 50   | Chess_Move_Accuracy: 21  | Move_Accuracy:  42.00\n2024-11-24 10-12 1.jpeg.csv    | Total_Rows: 48   | Matched_Rows: 18   | Total_Accuracy:  37.50 | Chess_Moves: 37   | Chess_Move_Accuracy: 14  | Move_Accuracy:  37.84\n2024-11-24 10-13 2.jpeg.csv    | Total_Rows: 109  | Matched_Rows: 39   | Total_Accuracy:  35.78 | Chess_Moves: 77   | Chess_Move_Accuracy: 31  | Move_Accuracy:  40.26\n20241124-1012.jpg.csv          | Total_Rows: 38   | Matched_Rows: 32   | Total_Accuracy:  84.21 | Chess_Moves: 27   | Chess_Move_Accuracy: 21  | Move_Accuracy:  77.78\n\nAccuracy results saved to: /kaggle/working/accuracy/accuracy_revision_5.csv\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"Accuracy code as function","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\ndef calculate_accuracy(directory_path, output_directory, preserved_boxes, epoch):\n    \"\"\"\n    Calculate accuracy metrics for OCR results and display combined metrics in table format.\n    \n    Args:\n        directory_path (str): Path to the directory containing CSV files.\n        output_directory (str): Path to save accuracy results.\n        preserved_boxes (list): List of preserved box numbers.\n        epoch (int): The current training epoch number.\n    \n    Returns:\n        str: Path to the saved combined accuracy results CSV file.\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_directory, exist_ok=True)\n\n    # Initialize variables for combined metrics\n    combined_total_rows = 0\n    combined_matches = 0\n    combined_preserved_rows = 0\n    combined_preserved_matches = 0\n\n    # Create a list to store file-specific results for CSV output\n    csv_output = []\n\n    # Iterate through all CSV files in the directory\n    for file_name in os.listdir(directory_path):\n        if file_name.endswith('.csv'):  # Process only CSV files\n            file_path = os.path.join(directory_path, file_name)\n            \n            try:\n                data = pd.read_csv(file_path)\n            except Exception as e:\n                print(f\"Error reading {file_name}: {e}\")\n                continue\n\n            # Check for required column\n            if 'Box Number' not in data.columns:\n                print(f\"Skipping {file_name}: 'Box Number' column not found.\")\n                continue\n\n            # Calculate Total Accuracy for the current file\n            data['Match'] = data['Original Text'] == data['Label']\n            total_rows = len(data)\n            matches = data['Match'].sum()\n            total_accuracy = (matches / total_rows) * 100 if total_rows > 0 else 0\n\n            # Filter rows for preserved boxes\n            preserved_data = data[data['Box Number'].isin(preserved_boxes)]\n            preserved_matches = (preserved_data['Original Text'] == preserved_data['Label']).sum()\n            preserved_rows = len(preserved_data)\n            move_accuracy = (preserved_matches / preserved_rows) * 100 if preserved_rows > 0 else 0\n\n            # Print results for the current file in table format\n            print(f\"{file_name:28} | Total_Rows: {total_rows:<4} | Matched_Rows: {matches:<4} | Total_Accuracy: {total_accuracy:6.2f} | Chess_Moves: {preserved_rows:<4} | Chess_Move_Accuracy: {preserved_matches:<4}| Move_Accuracy: {move_accuracy:6.2f}\")\n\n            # Add results to the CSV output list\n            csv_output.append({\n                \"Filename\": file_name,\n                \"Total_Rows\": total_rows,\n                \"Matches_Total_Accuracy\": matches,\n                \"Total_Accuracy\": total_accuracy,\n                \"Chess_Moves\": preserved_rows,\n                \"Matches_Move_Accuracy\": preserved_matches,\n                \"Move_Accuracy\": move_accuracy\n            })\n\n            # Update combined metrics\n            combined_total_rows += total_rows\n            combined_matches += matches\n            combined_preserved_rows += preserved_rows\n            combined_preserved_matches += preserved_matches\n\n    # Calculate combined metrics\n    combined_total_accuracy = (combined_matches / combined_total_rows) * 100 if combined_total_rows > 0 else 0\n    combined_move_accuracy = (combined_preserved_matches / combined_preserved_rows) * 100 if combined_preserved_rows > 0 else 0\n\n    # Print combined metrics in the specified structure\n    print(\"\\nCombined Metrics:\")\n    print(f\"{'Combined Total Rows':5} | Total_Rows: {combined_total_rows:<5} | Matches: {combined_matches:<5} | Total_Accuracy: {combined_total_accuracy:6.2f} | Chess_Moves: {combined_preserved_rows:<5} | Move_Accuracy: {combined_move_accuracy:6.2f}\")\n\n    # Save combined metrics to a separate CSV file for the epoch\n    combined_output_file = os.path.join(output_directory, f\"training_epoch{epoch}.csv\")\n    combined_metrics = {\n        \"Metric\": [\"Total Rows\", \"Total Matched Rows\", \"Total Accuracy\", \"Chess Moves\", \"Chess Move Matches\", \"Move Accuracy\"],\n        \"Value\": [combined_total_rows, combined_matches, combined_total_accuracy, combined_preserved_rows, combined_preserved_matches, combined_move_accuracy]\n    }\n    combined_df = pd.DataFrame(combined_metrics)\n    combined_df.to_csv(combined_output_file, index=False)\n    print(f\"\\nCombined metrics saved to: {combined_output_file}\")\n\n    # Save results to a CSV file for individual files\n    revision = 1\n    while os.path.exists(os.path.join(output_directory, f\"accuracy_revision_{revision}.csv\")):\n        revision += 1\n    output_file_path = os.path.join(output_directory, f\"accuracy_revision_{revision}.csv\")\n    csv_df = pd.DataFrame(csv_output)\n    csv_df.to_csv(output_file_path, index=False)\n    print(f\"\\nFile-specific accuracy results saved to: {output_file_path}\")\n\n    return combined_output_file\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T20:05:05.131281Z","iopub.execute_input":"2024-11-24T20:05:05.131780Z","iopub.status.idle":"2024-11-24T20:05:05.147863Z","shell.execute_reply.started":"2024-11-24T20:05:05.131736Z","shell.execute_reply":"2024-11-24T20:05:05.146581Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# ***Paddleocr recognition training***","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport paddle\n\nfrom paddleocr import PaddleOCRModel\n\n# Paths\nimage_dir = \"/kaggle/input/danny-chess-2\"\nlabel_dir = \"/kaggle/input/danny-chess-2\"\noutput_dir = \"/kaggle/working/paddleocr_training_output\"\ntesting_dir = \"/kaggle/input/danny-chess-2/testing\"\naccuracy_dir = \"/kaggle/working/accuracy\"\n\n# Ensure output directories exist\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(accuracy_dir, exist_ok=True)\n\n# Check for GPU availability\nuse_gpu = paddle.device.is_compiled_with_cuda()\nprint(f\"GPU Detected: {use_gpu}\")\n\n# Prepare Dataset\ndef load_csv_dataset(image_dir, label_dir):\n    dataset = []\n    for csv_file in os.listdir(label_dir):\n        if csv_file.endswith(\".csv\"):\n            image_name = csv_file.replace(\".csv\", \"\")\n            image_path = os.path.join(image_dir, image_name)\n            if os.path.exists(image_path):\n                labels = pd.read_csv(os.path.join(label_dir, csv_file))\n                for _, row in labels.iterrows():\n                    dataset.append({\n                        \"image\": image_path,\n                        \"coordinates\": [\n                            [row[\"x_min\"], row[\"y_min\"]],\n                            [row[\"x_max\"], row[\"y_min\"]],\n                            [row[\"x_max\"], row[\"y_max\"]],\n                            [row[\"x_min\"], row[\"y_max\"]]\n                        ],\n                        \"label\": row[\"label\"]\n                    })\n    return dataset\n\n# Load training dataset\ntraining_dataset = load_csv_dataset(image_dir, label_dir)\n\n# Define and Configure PaddleOCR Model\nocr_model = PaddleOCRModel(\n    rec_model_dir=\"/kaggle/working/en_PP-OCRv3_rec_infer\",  # Replace with your PaddleOCR model path or use default\n    det_model_dir=\"/kaggle/working/en_PP-OCRv3_det_infer\",  # Replace with your PaddleOCR detection model path\n    use_gpu=use_gpu  # Dynamically set GPU usage\n)\n\n# Accuracy Calculation Function\ndef calculate_accuracy(directory_path, output_directory):\n    os.makedirs(output_directory, exist_ok=True)\n    csv_output = []\n\n    # Iterate through all CSV files in the directory\n    for file_name in os.listdir(directory_path):\n        if file_name.endswith('.csv'):\n            file_path = os.path.join(directory_path, file_name)\n            try:\n                data = pd.read_csv(file_path)\n            except Exception as e:\n                print(f\"Error reading {file_name}: {e}\")\n                continue\n\n            if 'Box Number' not in data.columns:\n                print(f\"Skipping {file_name}: 'Box Number' column not found.\")\n                continue\n\n            data['Match'] = data['Original Text'] == data['Label']\n            total_rows = len(data)\n            matches = data['Match'].sum()\n            total_accuracy = (matches / total_rows) * 100 if total_rows > 0 else 0\n\n            preserved_data = data[data['Box Number'].isin(range(1, 10))]  # Example for preserved box numbers\n            preserved_matches = (preserved_data['Original Text'] == preserved_data['Label']).sum()\n            preserved_rows = len(preserved_data)\n            move_accuracy = (preserved_matches / preserved_rows) * 100 if preserved_rows > 0 else 0\n\n            csv_output.append({\n                \"Filename\": file_name,\n                \"Total_Rows\": total_rows,\n                \"Matches_Total_Accuracy\": matches,\n                \"Total_Accuracy\": total_accuracy,\n                \"Chess_Moves\": preserved_rows,\n                \"Matches_Move_Accuracy\": preserved_matches,\n                \"Move_Accuracy\": move_accuracy\n            })\n\n    revision = 1\n    while os.path.exists(os.path.join(output_directory, f\"accuracy_revision_{revision}.csv\")):\n        revision += 1\n    output_file_path = os.path.join(output_directory, f\"accuracy_revision_{revision}.csv\")\n    pd.DataFrame(csv_output).to_csv(output_file_path, index=False)\n    print(f\"Accuracy results saved to: {output_file_path}\")\n    return output_file_path\n\n# Training Function with Accuracy Calculation\ndef train_model(training_dataset, testing_dir, ocr_model, epochs=10, batch_size=16):\n    for epoch in range(epochs):\n        total_loss = 0\n        correct_predictions = 0\n        total_samples = 0\n\n        for data in training_dataset:\n            # Simulate training process\n            image = data[\"image\"]\n            coordinates = data[\"coordinates\"]\n            label = data[\"label\"]\n\n            # Perform training step here\n            # (In PaddleOCR, this is handled internally. Mock-up below.)\n            loss, predicted_label = ocr_model.train_step(image, coordinates, label)\n            total_loss += loss\n            if predicted_label == label:\n                correct_predictions += 1\n            total_samples += 1\n\n        # Metrics calculation\n        accuracy = correct_predictions / total_samples\n        avg_loss = total_loss / total_samples\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n        # Call calculate_accuracy for testing dataset\n        accuracy_file = calculate_accuracy(testing_dir, accuracy_dir)\n        print(f\"Validation accuracy saved at: {accuracy_file}\")\n\n# Train model\ntrain_model(training_dataset, testing_dir, ocr_model, epochs=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T20:05:24.023909Z","iopub.execute_input":"2024-11-24T20:05:24.024348Z","iopub.status.idle":"2024-11-24T20:05:24.136372Z","shell.execute_reply.started":"2024-11-24T20:05:24.024312Z","shell.execute_reply":"2024-11-24T20:05:24.134848Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpaddle\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaddleocr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaddleOCRModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[1;32m      8\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/danny-chess-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'PaddleOCRModel' from 'paddleocr' (/opt/conda/lib/python3.10/site-packages/paddleocr/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'PaddleOCRModel' from 'paddleocr' (/opt/conda/lib/python3.10/site-packages/paddleocr/__init__.py)","output_type":"error"}],"execution_count":41},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport glob\nimport ast\n\ndef normalize_name(name):\n    \"\"\"Normalize file name for consistent matching.\"\"\"\n    return os.path.splitext(name.replace(\" \", \"\").lower())[0]\n\ndef generate_detection_annotations(image_dir, output_txt_path):\n    \"\"\"\n    Generate detection annotations for all images and their corresponding CSV files in a directory.\n\n    Args:\n        image_dir (str): Directory containing images and their associated CSV files.\n        output_txt_path (str): Path to save the generated annotations in train.txt or test.txt format.\n    \"\"\"\n    annotation_lines = []\n\n    # Normalize all image files for comparison\n    image_files = {\n        normalize_name(file): file\n        for file in os.listdir(image_dir)\n        if file.lower().endswith(('.jpeg', '.jpg', '.png'))\n    }\n\n    # Find all CSV files in the directory\n    csv_files = glob.glob(os.path.join(image_dir, \"*.csv\"))\n\n    for csv_file in csv_files:\n        # Determine the base name (normalized) for matching\n        base_name = normalize_name(os.path.basename(csv_file).replace(\".csv\", \"\"))\n\n        # Find the corresponding image file\n        image_file = image_files.get(base_name)\n        if not image_file:\n            print(f\"Image not found for CSV: {csv_file}, skipping.\")\n            continue\n\n        # Construct the full image path\n        image_path = os.path.join(image_dir, image_file)\n\n        # Read the CSV and generate annotations\n        try:\n            data = pd.read_csv(csv_file)\n            annotations = []\n            for _, row in data.iterrows():\n                # Parse box coordinates and transcription\n                box_coordinates = ast.literal_eval(row[\"Box Coordinates\"])\n                annotations.append({\n                    \"transcription\": row[\"Label\"],\n                    \"points\": box_coordinates\n                })\n            # Append the annotation line for this image\n            annotation_lines.append(f'\"{image_path}\"\\t{annotations}')\n        except Exception as e:\n            print(f\"Error processing {csv_file}: {e}\")\n\n    # Save all annotations to the output file\n    with open(output_txt_path, \"w\") as f:\n        f.write(\"\\n\".join(annotation_lines))\n    print(f\"Annotations saved to: {output_txt_path}\")\n\n# Specify the training and testing directories and output paths\ntrain_image_dir = \"/kaggle/input/danny-chess-2\"\ntest_image_dir = \"/kaggle/input/danny-chess-2/testing\"\ntrain_output_path = \"/kaggle/working/train.txt\"\ntest_output_path = \"/kaggle/working/test.txt\"\n\n# Generate annotations for training and testing datasets\ngenerate_detection_annotations(train_image_dir, train_output_path)\ngenerate_detection_annotations(test_image_dir, test_output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:06:56.970535Z","iopub.execute_input":"2024-11-24T21:06:56.971384Z","iopub.status.idle":"2024-11-24T21:06:57.276479Z","shell.execute_reply.started":"2024-11-24T21:06:56.971341Z","shell.execute_reply":"2024-11-24T21:06:57.275361Z"}},"outputs":[{"name":"stdout","text":"Annotations saved to: /kaggle/working/train.txt\nAnnotations saved to: /kaggle/working/test.txt\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"Convert the data in the dataset to ICDAR2015 format for PaddleOCR training...","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport ast\n\n# Define source and destination paths\nsource_folder_training = '/kaggle/input/danny-chess-2'\nsource_folder_testing = '/kaggle/input/danny-chess-2/testing'\noutput_folder = '/kaggle/working/danny-chess-2/icdar2015'\n\n# Create output folder structure\noutput_training_images = os.path.join(output_folder, 'ch4_training_images')\noutput_training_gt = os.path.join(output_folder, 'ch4_training_localization_transcription_gt')\noutput_test_images = os.path.join(output_folder, 'ch4_test_images')\noutput_test_gt = os.path.join(output_folder, 'ch4_test_gt')\n\nos.makedirs(output_training_images, exist_ok=True)\nos.makedirs(output_training_gt, exist_ok=True)\nos.makedirs(output_test_images, exist_ok=True)\nos.makedirs(output_test_gt, exist_ok=True)\n\ndef process_dataset(source_folder, output_images_folder, output_gt_folder):\n    # Process all CSV files in the source folder\n    for root, _, files in os.walk(source_folder):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_path = os.path.join(root, file)\n                \n                # Use the full name of the image (including .jpeg or .jpg)\n                image_name = file[:-4]  # Remove '.csv' from the end of the file name\n                \n                # Check if the corresponding image exists\n                image_path = os.path.join(root, image_name)\n                if not os.path.exists(image_path):\n                    continue\n                \n                # Copy the image to the output images folder\n                output_image_path = os.path.join(output_images_folder, image_name)\n                if not os.path.exists(output_image_path):\n                    os.system(f\"cp '{image_path}' '{output_image_path}'\")\n                \n                # Read the CSV file\n                data = pd.read_csv(csv_path)\n                \n                # Prepare the ground truth file\n                base_name = os.path.splitext(image_name)[0]  # Strip the extension (e.g., .jpeg, .jpg)\n                gt_file_path = os.path.join(output_gt_folder, f\"gt_{base_name}.txt\")\n                with open(gt_file_path, 'w') as gt_file:\n                    for _, row in data.iterrows():\n                        # Parse the box coordinates\n                        box_coordinates = ast.literal_eval(row['Box Coordinates'])\n                        flattened_coords = [str(int(coord)) for point in box_coordinates for coord in point]\n                        \n                        # Combine coordinates and text\n                        text = row['Original Text'] if pd.notna(row['Original Text']) else \"###\"\n                        gt_file.write(','.join(flattened_coords) + f',\"{text}\"\\n')\n\n# Process training data\nprocess_dataset(source_folder_training, output_training_images, output_training_gt)\n\n# Process test data\nprocess_dataset(source_folder_testing, output_test_images, output_test_gt)\n\nprint(\"ICDAR2015 folder structure created successfully for both training and testing datasets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:55:17.148123Z","iopub.execute_input":"2024-11-24T21:55:17.148964Z","iopub.status.idle":"2024-11-24T21:55:17.686952Z","shell.execute_reply.started":"2024-11-24T21:55:17.148902Z","shell.execute_reply":"2024-11-24T21:55:17.685800Z"}},"outputs":[{"name":"stdout","text":"ICDAR2015 folder structure created successfully for both training and testing datasets.\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import yaml\n\n# Path to the existing YAML file\nyaml_path = \"/kaggle/working/det_mv3_db.yml\"\n\n# Load the YAML file\nwith open(yaml_path, 'r') as file:\n    config = yaml.safe_load(file)\n\n# Ensure 'dataset' and 'output' keys exist\nif 'dataset' not in config:\n    config['dataset'] = {}\nif 'output' not in config:\n    config['output'] = {}\n\n# Modify paths\nconfig['dataset']['training_images'] = \"/kaggle/working/danny-chess-2/icdar2015/ch4_training_images\"\nconfig['dataset']['training_annotations'] = \"/kaggle/working/danny-chess-2/icdar2015/ch4_training_localization_transcription_gt\"\nconfig['dataset']['test_images'] = \"/kaggle/working/danny-chess-2/icdar2015/ch4_test_images\"\nconfig['dataset']['test_annotations'] = \"/kaggle/working/danny-chess-2/icdar2015/ch4_test_gt\"\nconfig['output']['logs'] = \"/kaggle/working/output/logs\"\nconfig['output']['models'] = \"/kaggle/working/output/models\"\n\n# Save the modified YAML file\nwith open(yaml_path, 'w') as file:\n    yaml.dump(config, file)\n\nprint(f\"Updated YAML file saved to {yaml_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T22:17:34.699776Z","iopub.execute_input":"2024-11-24T22:17:34.700290Z","iopub.status.idle":"2024-11-24T22:17:34.741274Z","shell.execute_reply.started":"2024-11-24T22:17:34.700251Z","shell.execute_reply":"2024-11-24T22:17:34.740126Z"}},"outputs":[{"name":"stdout","text":"Updated YAML file saved to /kaggle/working/det_mv3_db.yml\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Path to the train.sh file\ntrain_sh_path = '/kaggle/working/PaddleOCR/train.sh'\n\n# Updated content for the train.sh file\nnew_content = \"\"\"#!/bin/bash\npython3 /kaggle/working/PaddleOCR/tools/train.py -c /kaggle/working/det_mv3_db.yml\npython3 /kaggle/working/PaddleOCR/tools/train.py -c /kaggle/working/rec_mv3_none_bilstm_ctc.yml\n\"\"\"\n\n# Write the new content to the train.sh file\nwith open(train_sh_path, 'w') as file:\n    file.write(new_content)\n\nprint(f\"Updated {train_sh_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:17:36.165753Z","iopub.execute_input":"2024-11-24T23:17:36.166862Z","iopub.status.idle":"2024-11-24T23:17:36.174703Z","shell.execute_reply.started":"2024-11-24T23:17:36.166811Z","shell.execute_reply":"2024-11-24T23:17:36.173466Z"}},"outputs":[{"name":"stdout","text":"Updated /kaggle/working/PaddleOCR/train.sh\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"with open('/kaggle/working/PaddleOCR/train.sh', 'r') as file:\n    print(file.read())\n!chmod +x /kaggle/working/PaddleOCR/train.sh\nfile_path = \"/kaggle/working/PaddleOCR/tools/program.py\"\n\n# Read the file\nwith open(file_path, 'r') as file:\n    lines = file.readlines()\n\n# Replace the problematic line\nlines = [\n    line.replace(\"dist.ParallelEnv().dev_id\", \"dist.ParallelEnv().device_id\")\n    if \"dist.ParallelEnv().dev_id\" in line else line\n    for line in lines\n]\n\n# Write the updated file\nwith open(file_path, 'w') as file:\n    file.writelines(lines)\n\nprint(f\"Updated {file_path} to replace 'dev_id' with 'device_id'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T22:31:04.971900Z","iopub.execute_input":"2024-11-24T22:31:04.972399Z","iopub.status.idle":"2024-11-24T22:31:06.104440Z","shell.execute_reply.started":"2024-11-24T22:31:04.972356Z","shell.execute_reply":"2024-11-24T22:31:06.102780Z"}},"outputs":[{"name":"stdout","text":"#!/bin/bash\npython3 /kaggle/working/PaddleOCR/tools/train.py -c /kaggle/working/det_mv3_db.yml\n\nUpdated /kaggle/working/PaddleOCR/tools/program.py to replace 'dev_id' with 'device_id'\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"#update detection yml file\nimport yaml\n\n# Path to the YAML file\nyaml_path = \"/kaggle/working/det_mv3_db.yml\"\n\n# Load the YAML file\nwith open(yaml_path, 'r') as file:\n    config = yaml.safe_load(file)\n\n# Ensure 'Global', 'Train', 'Optimizer', and 'PostProcess' sections exist\nconfig.setdefault('Global', {})\nconfig.setdefault('Train', {})\nconfig.setdefault('Optimizer', {})\nconfig.setdefault('PostProcess', {})\nconfig['Train'].setdefault('loader', {})\nconfig['Optimizer'].setdefault('lr', {})\n\n# Update 'Global' section\nconfig['Global']['epoch_num'] = 100\nconfig['Global']['cal_metric_during_train'] = True\nconfig['Global']['save_epoch_step'] = 20\nconfig['Global']['eval_batch_step'] = [0, 50]\nconfig['Global']['pretrained_model'] = './pretrain_models/MobileNetV3_large_x0_5_pretrained'\n\n# Update 'Train' loader settings\nconfig['Train']['loader']['batch_size_per_card'] = 2\nconfig['Train']['loader']['shuffle'] = True\nconfig['Train']['loader']['drop_last'] = False\n\n# Update 'Optimizer' settings\nconfig['Optimizer']['lr']['name'] = 'Cosine'\nconfig['Optimizer']['lr']['learning_rate'] = 0.0005\nconfig['Optimizer']['lr']['warmup_epoch'] = 5\nconfig['Optimizer']['regularizer'] = {'name': 'L2', 'factor': 0}\n\n# Update 'PostProcess' settings\nconfig['PostProcess']['box_thresh'] = 0.5\nconfig['PostProcess']['unclip_ratio'] = 1.7\n\n# Update 'Architecture' settings for small dataset\nconfig.setdefault('Architecture', {})\nconfig['Architecture'].setdefault('Backbone', {})\nconfig['Architecture'].setdefault('Neck', {})\nconfig['Architecture']['Backbone']['model_name'] = 'large'\nconfig['Architecture']['Backbone']['name'] = 'MobileNetV3'\nconfig['Architecture']['Backbone']['scale'] = 0.5  # Keep this as 0.5 for now\nconfig['Architecture']['Neck']['name'] = 'DBFPN'\nconfig['Architecture']['Neck']['out_channels'] = 128\n\n# Save the modified YAML file\nwith open(yaml_path, 'w') as file:\n    yaml.dump(config, file, default_flow_style=False)\n\nprint(f\"Updated {yaml_path} with recommended settings for a small dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:15:18.681171Z","iopub.execute_input":"2024-11-24T23:15:18.682022Z","iopub.status.idle":"2024-11-24T23:15:18.724723Z","shell.execute_reply.started":"2024-11-24T23:15:18.681981Z","shell.execute_reply":"2024-11-24T23:15:18.723607Z"}},"outputs":[{"name":"stdout","text":"Updated /kaggle/working/det_mv3_db.yml with recommended settings for a small dataset.\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"#!cat  \"/kaggle/working/det_mv3_db.yml\"\n#!find /kaggle/working/ -name \"rec_mv3_none_bilstm_ctc.yml\n#!cp /kaggle/working/PaddleOCR/configs/rec/rec_mv3_none_bilstm_ctc.yml /kaggle/working/rec_mv3_none_bilstm_ctc.yml\n#!cat /kaggle/working/PaddleOCR/train.sh\n#!ls /kaggle/working/PaddleOCR/configs/rec/\n!cat /kaggle/working/rec_mv3_none_bilstm_ctc.yml","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"#!/bin/bash\npython3 /kaggle/working/PaddleOCR/tools/train.py -c /kaggle/working/det_mv3_db.yml\npython3 /kaggle/working/PaddleOCR/tools/train.py -c /kaggle/working/rec_mv3_none_bilstm_ctc.yml\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"#modify recognition yml\nimport yaml\n\n# Path to the YAML file\nyaml_path = \"/kaggle/working/rec_mv3_none_bilstm_ctc.yml\"\n\n# Load the existing YAML file\nwith open(yaml_path, 'r') as file:\n    config = yaml.safe_load(file)\n\n# Update only the necessary fields\n# Global settings\nconfig['Global']['epoch_num'] = 100  # Set epochs to 100\nconfig['Global']['pretrained_model'] = \"./pretrain_models/MobileNetV3_large_x0_5_pretrained\"\nconfig['Global']['character_dict_path'] = \"./ppocr/utils/dict/en_dict.txt\"\nconfig['Global']['save_model_dir'] = \"./output/rec/mv3_none_bilstm_ctc/\"\nconfig['Global']['eval_batch_step'] = [0, 2000]  # Evaluate every 2000 iterations\nconfig['Global']['cal_metric_during_train'] = True\n\n# Train dataset settings\nconfig['Train']['dataset']['data_dir'] = \"/kaggle/working/danny-chess-2/icdar2015/ch4_training_images\"\nconfig['Train']['loader']['batch_size_per_card'] = 64  # Adjust batch size\n\n# Eval dataset settings\nconfig['Eval']['dataset']['data_dir'] = \"/kaggle/working/danny-chess-2/icdar2015/ch4_test_images\"\nconfig['Eval']['loader']['batch_size_per_card'] = 32  # Adjust batch size for evaluation\n\n# Save the modified YAML file\nwith open(yaml_path, 'w') as file:\n    yaml.dump(config, file, default_flow_style=False)\n\nprint(f\"Updated {yaml_path} with necessary changes.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:15:29.960248Z","iopub.execute_input":"2024-11-24T23:15:29.960652Z","iopub.status.idle":"2024-11-24T23:15:29.988227Z","shell.execute_reply.started":"2024-11-24T23:15:29.960619Z","shell.execute_reply":"2024-11-24T23:15:29.986996Z"}},"outputs":[{"name":"stdout","text":"Updated /kaggle/working/rec_mv3_none_bilstm_ctc.yml with necessary changes.\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"!bash /kaggle/working/PaddleOCR/train.sh\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T22:33:12.981873Z","iopub.execute_input":"2024-11-24T22:33:12.982780Z","iopub.status.idle":"2024-11-24T22:40:54.552030Z","shell.execute_reply.started":"2024-11-24T22:33:12.982737Z","shell.execute_reply":"2024-11-24T22:40:54.542986Z"}},"outputs":[{"name":"stdout","text":"[2024/11/24 22:33:17] ppocr INFO: Architecture : \n[2024/11/24 22:33:17] ppocr INFO:     Backbone : \n[2024/11/24 22:33:17] ppocr INFO:         model_name : large\n[2024/11/24 22:33:17] ppocr INFO:         name : MobileNetV3\n[2024/11/24 22:33:17] ppocr INFO:         scale : 0.5\n[2024/11/24 22:33:17] ppocr INFO:     Head : \n[2024/11/24 22:33:17] ppocr INFO:         k : 50\n[2024/11/24 22:33:17] ppocr INFO:         name : DBHead\n[2024/11/24 22:33:17] ppocr INFO:     Neck : \n[2024/11/24 22:33:17] ppocr INFO:         name : DBFPN\n[2024/11/24 22:33:17] ppocr INFO:         out_channels : 256\n[2024/11/24 22:33:17] ppocr INFO:     Transform : None\n[2024/11/24 22:33:17] ppocr INFO:     algorithm : DB\n[2024/11/24 22:33:17] ppocr INFO:     model_type : det\n[2024/11/24 22:33:17] ppocr INFO: Eval : \n[2024/11/24 22:33:17] ppocr INFO:     dataset : \n[2024/11/24 22:33:17] ppocr INFO:         data_dir : ./Sample-Data/Test/\n[2024/11/24 22:33:17] ppocr INFO:         label_file_list : ['./Sample-Data/Test/test.txt']\n[2024/11/24 22:33:17] ppocr INFO:         name : SimpleDataSet\n[2024/11/24 22:33:17] ppocr INFO:         transforms : \n[2024/11/24 22:33:17] ppocr INFO:             DecodeImage : \n[2024/11/24 22:33:17] ppocr INFO:                 channel_first : False\n[2024/11/24 22:33:17] ppocr INFO:                 img_mode : BGR\n[2024/11/24 22:33:17] ppocr INFO:             DetLabelEncode : None\n[2024/11/24 22:33:17] ppocr INFO:             DetResizeForTest : \n[2024/11/24 22:33:17] ppocr INFO:                 image_shape : [736, 1280]\n[2024/11/24 22:33:17] ppocr INFO:             NormalizeImage : \n[2024/11/24 22:33:17] ppocr INFO:                 mean : [0.485, 0.456, 0.406]\n[2024/11/24 22:33:17] ppocr INFO:                 order : hwc\n[2024/11/24 22:33:17] ppocr INFO:                 scale : 1./255.\n[2024/11/24 22:33:17] ppocr INFO:                 std : [0.229, 0.224, 0.225]\n[2024/11/24 22:33:17] ppocr INFO:             ToCHWImage : None\n[2024/11/24 22:33:17] ppocr INFO:             KeepKeys : \n[2024/11/24 22:33:17] ppocr INFO:                 keep_keys : ['image', 'shape', 'polys', 'ignore_tags']\n[2024/11/24 22:33:17] ppocr INFO:     loader : \n[2024/11/24 22:33:17] ppocr INFO:         batch_size_per_card : 1\n[2024/11/24 22:33:17] ppocr INFO:         drop_last : False\n[2024/11/24 22:33:17] ppocr INFO:         num_workers : 8\n[2024/11/24 22:33:17] ppocr INFO:         shuffle : False\n[2024/11/24 22:33:17] ppocr INFO:         use_shared_memory : True\n[2024/11/24 22:33:17] ppocr INFO: Global : \n[2024/11/24 22:33:17] ppocr INFO:     cal_metric_during_train : False\n[2024/11/24 22:33:17] ppocr INFO:     checkpoints : None\n[2024/11/24 22:33:17] ppocr INFO:     distributed : False\n[2024/11/24 22:33:17] ppocr INFO:     epoch_num : 500\n[2024/11/24 22:33:17] ppocr INFO:     eval_batch_step : [0, 100]\n[2024/11/24 22:33:17] ppocr INFO:     infer_img : ./Sample-Data/Train/11664.jpg\n[2024/11/24 22:33:17] ppocr INFO:     log_smooth_window : 20\n[2024/11/24 22:33:17] ppocr INFO:     pretrained_model : ./pretrain_models/MobileNetV3_large_x0_5_pretrained\n[2024/11/24 22:33:17] ppocr INFO:     print_batch_step : 10\n[2024/11/24 22:33:17] ppocr INFO:     save_epoch_step : 500\n[2024/11/24 22:33:17] ppocr INFO:     save_inference_dir : None\n[2024/11/24 22:33:17] ppocr INFO:     save_model_dir : ./output/db_mv3/\n[2024/11/24 22:33:17] ppocr INFO:     save_res_path : ./output/det_db/predicts_db.txt\n[2024/11/24 22:33:17] ppocr INFO:     use_gpu : False\n[2024/11/24 22:33:17] ppocr INFO:     use_mlu : False\n[2024/11/24 22:33:17] ppocr INFO:     use_visualdl : False\n[2024/11/24 22:33:17] ppocr INFO:     use_xpu : False\n[2024/11/24 22:33:17] ppocr INFO: Loss : \n[2024/11/24 22:33:17] ppocr INFO:     alpha : 5\n[2024/11/24 22:33:17] ppocr INFO:     balance_loss : True\n[2024/11/24 22:33:17] ppocr INFO:     beta : 10\n[2024/11/24 22:33:17] ppocr INFO:     main_loss_type : DiceLoss\n[2024/11/24 22:33:17] ppocr INFO:     name : DBLoss\n[2024/11/24 22:33:17] ppocr INFO:     ohem_ratio : 3\n[2024/11/24 22:33:17] ppocr INFO: Metric : \n[2024/11/24 22:33:17] ppocr INFO:     main_indicator : hmean\n[2024/11/24 22:33:17] ppocr INFO:     name : DetMetric\n[2024/11/24 22:33:17] ppocr INFO: Optimizer : \n[2024/11/24 22:33:17] ppocr INFO:     beta1 : 0.9\n[2024/11/24 22:33:17] ppocr INFO:     beta2 : 0.999\n[2024/11/24 22:33:17] ppocr INFO:     lr : \n[2024/11/24 22:33:17] ppocr INFO:         learning_rate : 0.001\n[2024/11/24 22:33:17] ppocr INFO:     name : Adam\n[2024/11/24 22:33:17] ppocr INFO:     regularizer : \n[2024/11/24 22:33:17] ppocr INFO:         factor : 0\n[2024/11/24 22:33:17] ppocr INFO:         name : L2\n[2024/11/24 22:33:17] ppocr INFO: PostProcess : \n[2024/11/24 22:33:17] ppocr INFO:     box_thresh : 0.6\n[2024/11/24 22:33:17] ppocr INFO:     max_candidates : 1000\n[2024/11/24 22:33:17] ppocr INFO:     name : DBPostProcess\n[2024/11/24 22:33:17] ppocr INFO:     thresh : 0.3\n[2024/11/24 22:33:17] ppocr INFO:     unclip_ratio : 1.5\n[2024/11/24 22:33:17] ppocr INFO: Train : \n[2024/11/24 22:33:17] ppocr INFO:     dataset : \n[2024/11/24 22:33:17] ppocr INFO:         data_dir : ./Sample-Data/Train/\n[2024/11/24 22:33:17] ppocr INFO:         label_file_list : ['./Sample-Data/Train/train.txt']\n[2024/11/24 22:33:17] ppocr INFO:         name : SimpleDataSet\n[2024/11/24 22:33:17] ppocr INFO:         ratio_list : [1.0]\n[2024/11/24 22:33:17] ppocr INFO:         transforms : \n[2024/11/24 22:33:17] ppocr INFO:             DecodeImage : \n[2024/11/24 22:33:17] ppocr INFO:                 channel_first : False\n[2024/11/24 22:33:17] ppocr INFO:                 img_mode : BGR\n[2024/11/24 22:33:17] ppocr INFO:             DetLabelEncode : None\n[2024/11/24 22:33:17] ppocr INFO:             IaaAugment : \n[2024/11/24 22:33:17] ppocr INFO:                 augmenter_args : \n[2024/11/24 22:33:17] ppocr INFO:                     args : \n[2024/11/24 22:33:17] ppocr INFO:                         p : 0.5\n[2024/11/24 22:33:17] ppocr INFO:                     type : Fliplr\n[2024/11/24 22:33:17] ppocr INFO:                     args : \n[2024/11/24 22:33:17] ppocr INFO:                         rotate : [-10, 10]\n[2024/11/24 22:33:17] ppocr INFO:                     type : Affine\n[2024/11/24 22:33:17] ppocr INFO:                     args : \n[2024/11/24 22:33:17] ppocr INFO:                         size : [0.5, 3]\n[2024/11/24 22:33:17] ppocr INFO:                     type : Resize\n[2024/11/24 22:33:17] ppocr INFO:             EastRandomCropData : \n[2024/11/24 22:33:17] ppocr INFO:                 keep_ratio : True\n[2024/11/24 22:33:17] ppocr INFO:                 max_tries : 50\n[2024/11/24 22:33:17] ppocr INFO:                 size : [640, 640]\n[2024/11/24 22:33:17] ppocr INFO:             MakeBorderMap : \n[2024/11/24 22:33:17] ppocr INFO:                 shrink_ratio : 0.4\n[2024/11/24 22:33:17] ppocr INFO:                 thresh_max : 0.7\n[2024/11/24 22:33:17] ppocr INFO:                 thresh_min : 0.3\n[2024/11/24 22:33:17] ppocr INFO:             MakeShrinkMap : \n[2024/11/24 22:33:17] ppocr INFO:                 min_text_size : 8\n[2024/11/24 22:33:17] ppocr INFO:                 shrink_ratio : 0.4\n[2024/11/24 22:33:17] ppocr INFO:             NormalizeImage : \n[2024/11/24 22:33:17] ppocr INFO:                 mean : [0.485, 0.456, 0.406]\n[2024/11/24 22:33:17] ppocr INFO:                 order : hwc\n[2024/11/24 22:33:17] ppocr INFO:                 scale : 1./255.\n[2024/11/24 22:33:17] ppocr INFO:                 std : [0.229, 0.224, 0.225]\n[2024/11/24 22:33:17] ppocr INFO:             ToCHWImage : None\n[2024/11/24 22:33:17] ppocr INFO:             KeepKeys : \n[2024/11/24 22:33:17] ppocr INFO:                 keep_keys : ['image', 'threshold_map', 'threshold_mask', 'shrink_map', 'shrink_mask']\n[2024/11/24 22:33:17] ppocr INFO:     loader : \n[2024/11/24 22:33:17] ppocr INFO:         batch_size_per_card : 5\n[2024/11/24 22:33:17] ppocr INFO:         drop_last : False\n[2024/11/24 22:33:17] ppocr INFO:         num_workers : 8\n[2024/11/24 22:33:17] ppocr INFO:         shuffle : True\n[2024/11/24 22:33:17] ppocr INFO:         use_shared_memory : True\n[2024/11/24 22:33:17] ppocr INFO: dataset : \n[2024/11/24 22:33:17] ppocr INFO:     test_annotations : /kaggle/working/danny-chess-2/icdar2015/ch4_test_gt\n[2024/11/24 22:33:17] ppocr INFO:     test_images : /kaggle/working/danny-chess-2/icdar2015/ch4_test_images\n[2024/11/24 22:33:17] ppocr INFO:     training_annotations : /kaggle/working/danny-chess-2/icdar2015/ch4_training_localization_transcription_gt\n[2024/11/24 22:33:17] ppocr INFO:     training_images : /kaggle/working/danny-chess-2/icdar2015/ch4_training_images\n[2024/11/24 22:33:17] ppocr INFO: output : \n[2024/11/24 22:33:17] ppocr INFO:     logs : /kaggle/working/output/logs\n[2024/11/24 22:33:17] ppocr INFO:     models : /kaggle/working/output/models\n[2024/11/24 22:33:17] ppocr INFO: profiler_options : None\n[2024/11/24 22:33:17] ppocr INFO: train with paddle 2.6.2 and device Place(cpu)\n[2024/11/24 22:33:17] ppocr INFO: Initialize indexs of datasets:['./Sample-Data/Train/train.txt']\n[2024/11/24 22:33:17] ppocr INFO: Initialize indexs of datasets:['./Sample-Data/Test/test.txt']\n[2024/11/24 22:33:18] ppocr INFO: train dataloader has 3 iters\n[2024/11/24 22:33:18] ppocr INFO: valid dataloader has 6 iters\n[2024/11/24 22:33:18] ppocr INFO: load pretrain successful from ./pretrain_models/MobileNetV3_large_x0_5_pretrained\n[2024/11/24 22:33:18] ppocr INFO: During the training process, after the 0th iteration, an evaluation is run every 100 iterations\n[2024/11/24 22:35:09] ppocr INFO: epoch: [1/500], global_step: 3, lr: 0.001000, loss: 9.168466, loss_shrink_maps: 4.649062, loss_threshold_maps: 3.567381, loss_binary_maps: 0.932017, loss_cbn: 0.000000, avg_reader_cost: 0.24627 s, avg_batch_cost: 11.16047 s, avg_samples: 1.5, ips: 0.13440 samples/s, eta: 15:28:10\n[2024/11/24 22:35:09] ppocr INFO: save model in ./output/db_mv3/latest\n[2024/11/24 22:36:41] ppocr INFO: epoch: [2/500], global_step: 6, lr: 0.001000, loss: 8.506873, loss_shrink_maps: 4.494134, loss_threshold_maps: 3.080364, loss_binary_maps: 0.911009, loss_cbn: 0.000000, avg_reader_cost: 0.37573 s, avg_batch_cost: 9.21358 s, avg_samples: 1.5, ips: 0.16280 samples/s, eta: 14:05:31\n[2024/11/24 22:36:42] ppocr INFO: save model in ./output/db_mv3/latest\n[2024/11/24 22:38:09] ppocr INFO: epoch: [3/500], global_step: 9, lr: 0.001000, loss: 7.739490, loss_shrink_maps: 4.434038, loss_threshold_maps: 2.526545, loss_binary_maps: 0.893849, loss_cbn: 0.000000, avg_reader_cost: 0.32970 s, avg_batch_cost: 8.80529 s, avg_samples: 1.5, ips: 0.17035 samples/s, eta: 13:25:40\n[2024/11/24 22:38:10] ppocr INFO: save model in ./output/db_mv3/latest\n[2024/11/24 22:38:42] ppocr INFO: epoch: [4/500], global_step: 10, lr: 0.001000, loss: 7.414585, loss_shrink_maps: 4.393002, loss_threshold_maps: 2.216748, loss_binary_maps: 0.889046, loss_cbn: 0.000000, avg_reader_cost: 0.32772 s, avg_batch_cost: 3.29469 s, avg_samples: 0.5, ips: 0.15176 samples/s, eta: 13:26:26\n[2024/11/24 22:39:38] ppocr INFO: epoch: [4/500], global_step: 12, lr: 0.001000, loss: 7.014788, loss_shrink_maps: 4.345639, loss_threshold_maps: 1.757039, loss_binary_maps: 0.880463, loss_cbn: 0.000000, avg_reader_cost: 0.00042 s, avg_batch_cost: 5.57424 s, avg_samples: 1.0, ips: 0.17940 samples/s, eta: 13:06:19\n[2024/11/24 22:39:39] ppocr INFO: save model in ./output/db_mv3/latest\n^C\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"!rm -rf /kaggle/working/danny-chess-2/icdar2015\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:54:28.856544Z","iopub.execute_input":"2024-11-24T21:54:28.857047Z","iopub.status.idle":"2024-11-24T21:54:30.013107Z","shell.execute_reply.started":"2024-11-24T21:54:28.857008Z","shell.execute_reply":"2024-11-24T21:54:30.011293Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"!python3 /kaggle/working/PaddleOCR/tools/train.py \\\n    -c /kaggle/working/placeholder_rec_train.yml \\\n    --epoch_num 100 \\\n    --save_model_dir /kaggle/working/paddleocr_output \\\n    --device gpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from paddleocr import PaddleOCR\nimport torch.nn as nn\nfrom torchmetrics import Accuracy, Precision, Recall, F1Score, AveragePrecision\n\nclass ChessSequenceModel(nn.Module):\n    def __init__(self, num_classes, max_seq_length=100):\n        super(ChessSequenceModel, self).__init__()\n        self.max_seq_length = max_seq_length\n        # Initialize PaddleOCR\n        self.paddleocr = PaddleOCR(use_angle_cls=True, lang=\"en\")\n        \n    def forward(self, x):\n        batch_results = []\n        for image in x:  # Process batch of images\n            # Convert PyTorch tensor to NumPy array (HWC, uint8)\n            image_np = (image.permute(1, 2, 0).numpy() * 255).astype('uint8')\n            \n            # Use PaddleOCR for text detection and recognition\n            results = self.paddleocr.ocr(image_np, cls=True)\n            \n            # Extract recognized text (sequence of chess moves)\n            sequence = [res[1][0] for res in results[0]]  # Extract text from results\n            \n            batch_results.append(sequence)\n        \n        return batch_results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**let's save the model if we get restarted**","metadata":{}},{"cell_type":"code","source":"#save the trained model /kaggle/working/paddleocr_training_output/trained_model_weights\nocr_model.save_weights(\"/kaggle/working/paddleocr_training_output/trained_model_weights\")\nprint(\"Trained model weights saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"reload the trained model","metadata":{}},{"cell_type":"code","source":"#load saved mode from /kaggle/working/paddleocr_training_output/trained_model_weights. Must be saved first\nfrom paddleocr import PaddleOCRModel\n\n# Paths to the trained weights and the default pre-trained weights\ntrained_model_dir = \"/kaggle/working/paddleocr_training_output/trained_model_weights\"\nstock_rec_model_dir = \"/kaggle/working/en_PP-OCRv3_rec_infer\"\nstock_det_model_dir = \"/kaggle/working/en_PP-OCRv3_det_infer\"\n\n# Reload the trained model\nocr_model_trained = PaddleOCRModel(\n    rec_model_dir=trained_model_dir,\n    det_model_dir=trained_model_dir,  # Adjust if separate trained detection weights are saved\n    use_gpu=paddle.device.is_compiled_with_cuda()\n)\n\n# Initialize the stock PaddleOCR model\nocr_model_stock = PaddleOCRModel(\n    rec_model_dir=stock_rec_model_dir,\n    det_model_dir=stock_det_model_dir,\n    use_gpu=paddle.device.is_compiled_with_cuda()\n)\n\nprint(\"Trained and stock PaddleOCR models loaded successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def __getitem__(self, idx):\n    image_path, moves = self.image_labels[idx]\n    image = Image.open(image_path).convert(\"RGB\")\n    image = np.array(image)\n    \n    # Resize for PaddleOCR (default sizes work well)\n    if image.ndim == 2:\n        image = np.expand_dims(image, axis=-1)\n        image = np.repeat(image, 3, axis=-1)\n\n    # Convert image to tensor\n    image = torch.from_numpy(image.transpose((2, 0, 1))).float() / 255.0\n\n    # Labels as tensor for evaluation\n    labels = convert_labels_to_tensor(moves, self.move_to_idx)\n    return image, labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch_idx, (images, labels, lengths, max_len) in enumerate(dataloader):\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    # Forward pass through PaddleOCR-based model\n    predictions = model(images)\n    \n    # Convert predictions to indices\n    pred_indices = [\n        [move_to_idx.get(move.lower(), padding_idx) for move in sequence]\n        for sequence in predictions\n    ]\n    \n    # Pad predictions to match labels\n    pred_indices = pad_sequence(\n        [torch.tensor(seq) for seq in pred_indices], batch_first=True, padding_value=padding_idx\n    ).to(device)\n    \n    # Calculate loss\n    labels_reshaped = labels.view(-1)\n    valid_mask = labels_reshaped != padding_idx\n    loss = criterion(pred_indices.view(-1, len(move_to_idx))[valid_mask], labels_reshaped[valid_mask])\n    \n    loss.backward()\n    optimizer.step()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the model\nmodel = ChessSequenceModel(num_classes=100, max_seq_length=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom paddleocr import PaddleOCR\n\nclass ChessNotationDataset(Dataset):\n    def __init__(self, image_folder, label_file, transform=None):\n        \"\"\"\n        Dataset class for loading chessboard images and corresponding moves.\n\n        Args:\n            image_folder (str): Path to the folder containing images.\n            label_file (str): Path to the label file mapping images to moves.\n            transform (callable, optional): Transformation to apply to images.\n        \"\"\"\n        self.image_folder = image_folder\n        self.transform = transform\n        self.image_labels = self._parse_labels(label_file)\n\n    def _parse_labels(self, label_file):\n        \"\"\"\n        Parse the label file to create a mapping of images to moves.\n\n        Args:\n            label_file (str): Path to the label file.\n\n        Returns:\n            list of tuples: Each tuple contains the image path and the associated moves.\n        \"\"\"\n        image_labels = []\n        with open(label_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                image_name = parts[0]\n                moves = parts[1:]\n                image_path = os.path.join(self.image_folder, image_name)\n                if os.path.exists(image_path):\n                    image_labels.append((image_path, moves))\n        return image_labels\n\n    def __len__(self):\n        return len(self.image_labels)\n\n    def __getitem__(self, idx):\n        image_path, moves = self.image_labels[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # Apply transformations if provided\n        if self.transform:\n            image = self.transform(image)\n        else:\n            # Default: Convert image to PyTorch tensor (HWC -> CHW)\n            image = torch.from_numpy(np.array(image).transpose((2, 0, 1))).float() / 255.0\n\n        return image, moves\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# Define paths\nimage_folder = \"/kaggle/input/chess-dataset-notation/data\"\nlabel_file = \"/kaggle/input/chess-dataset-notation/labels.txt\"  # Replace with actual label file path\n\n# Define dataset and transformations\ntransform = transforms.Compose([\n    transforms.Resize((299, 299)),  # Resize to 299x299 for PaddleOCR compatibility\n    transforms.ToTensor(),          # Convert image to tensor\n])\n\ndataset = ChessNotationDataset(image_folder=image_folder, label_file=label_file, transform=transform)\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -la /kaggle/input/chess-dataset-notation/data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Function to split png file into rows","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\n\ndef split_by_horizontal_lines_sequential(image_path, output_folder, row_prefix, line_offset=2, min_row_height=10):\n    \"\"\"\n    Split an image into rows based on horizontal lines, excluding lines themselves, with sequential numbering.\n\n    Args:\n        image_path (str): Path to the PNG file.\n        output_folder (str): Folder to save cropped rows.\n        row_prefix (str): Prefix for naming cropped regions.\n        line_offset (int): Number of pixels to offset the crop region to exclude lines.\n        min_row_height (int): Minimum height in pixels for a row to be considered valid.\n\n    Returns:\n        list of str: Filenames of cropped rows.\n    \"\"\"\n    # Load image\n    image = cv2.imread(image_path)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Edge detection to highlight lines\n    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n\n    # Detect lines using Hough Transform\n    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi / 180, threshold=100,\n                            minLineLength=100, maxLineGap=10)\n\n    # Extract horizontal lines (angle ~ 0 degrees)\n    horizontal_lines = []\n    for line in lines:\n        x1, y1, x2, y2 = line[0]\n        if abs(y1 - y2) < 5:  # Check if the line is horizontal\n            horizontal_lines.append((x1, y1, x2, y2))\n\n    # Sort horizontal lines by their y-coordinates\n    horizontal_lines = sorted(horizontal_lines, key=lambda line: line[1])\n\n    # Split the image into rows using horizontal lines\n    row_files = []\n    row_counter = 1  # Sequential counter for rows\n    for i in range(len(horizontal_lines) - 1):\n        _, y1, _, _ = horizontal_lines[i]\n        _, y2, _, _ = horizontal_lines[i + 1]\n\n        # Exclude lines by adding an offset\n        y1 = y1 + line_offset  # Start just below the current line\n        y2 = y2 - line_offset  # End just above the next line\n\n        # Validate y-coordinates\n        y1 = max(0, y1)\n        y2 = min(gray.shape[0], y2)\n        if y1 >= y2 or (y2 - y1) < min_row_height:  # Check minimum row height\n            print(f\"Skipping invalid or small row: y1={y1}, y2={y2}, height={y2 - y1}\")\n            continue\n\n        # Crop the region\n        cropped_row = image[y1:y2, :]\n        if cropped_row.size == 0:\n            print(f\"Empty cropped row for y1={y1}, y2={y2}, skipping...\")\n            continue\n\n        # Save the cropped row with sequential numbering\n        row_filename = f\"{row_prefix}_row_{row_counter}.png\"\n        cv2.imwrite(os.path.join(output_folder, row_filename), cropped_row)\n        row_files.append(row_filename)\n        row_counter += 1  # Increment the row counter\n\n    # Save a debug image with horizontal lines drawn\n    debug_image = image.copy()\n    for x1, y1, x2, y2 in horizontal_lines:\n        cv2.line(debug_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n    debug_filename = os.path.join(output_folder, f\"{row_prefix}_debug.png\")\n    cv2.imwrite(debug_filename, debug_image)\n\n    return row_files\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"define a horizontal split function with one file","metadata":{}},{"cell_type":"code","source":"# Example Usage\nimage_path = \"/kaggle/input/chess-dataset-notation/data/153_0.png\"\noutput_folder = \"/kaggle/working/cropped_rows\"\nos.makedirs(output_folder, exist_ok=True)\n\nrow_prefix = \"153_0\"  # Matches the prefix in training_tags.txt\nboard_files = split_by_horizontal_lines_sequential(\n    image_path=image_path,\n    output_folder=output_folder,\n    row_prefix=row_prefix\n    \n#    min_width=250,         # 1.5 inches at 96 DPI\n#    min_height=50,         # 0.25 inches at 96 DPI\n#    max_region_ratio=0.3,  # Exclude regions larger than 90% of the image\n#    padding=5             # Extra padding\n)\n\nprint(\"Extracted chessboards:\", board_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -r /kaggle/working/cropped_rows","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"function for vertical line split","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\n\ndef detect_and_split_columns_debug(row_image_path, output_folder, column_prefix, line_offset=2, min_column_width=10):\n    \"\"\"\n    Detect vertical lines in a row and split it into columns, with debugging output.\n\n    Args:\n        row_image_path (str): Path to the row image.\n        output_folder (str): Folder to save debug and cropped columns.\n        column_prefix (str): Prefix for naming cropped columns.\n        line_offset (int): Number of pixels to offset the crop region to exclude lines.\n        min_column_width (int): Minimum width in pixels for a valid column.\n\n    Returns:\n        list of str: Filenames of the cropped columns.\n    \"\"\"\n    # Load row image\n    image = cv2.imread(row_image_path)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Step 1: Edge Detection\n    edges = cv2.Canny(gray, 20, 80, apertureSize=3)\n    edge_debug_path = os.path.join(output_folder, \"edges_debug.png\")\n    cv2.imwrite(edge_debug_path, edges)\n    print(f\"Saved edge detection debug image: {edge_debug_path}\")\n\n    # Step 2: Detect Lines\n    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi / 180, threshold=30,\n                            minLineLength=50, maxLineGap=5)\n\n    # Debugging: Draw detected lines\n    debug_image = image.copy()\n    vertical_lines = []\n    if lines is not None:\n        for line in lines:\n            x1, y1, x2, y2 = line[0]\n            if abs(x1 - x2) < 10:  # Vertical lines have small x-differences\n                vertical_lines.append((x1, y1, x2, y2))\n                cv2.line(debug_image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw line\n    else:\n        print(\"No vertical lines detected.\")\n\n    debug_path = os.path.join(output_folder, \"lines_debug.png\")\n    cv2.imwrite(debug_path, debug_image)\n    print(f\"Saved line detection debug image: {debug_path}\")\n\n    # Step 3: Sort and Validate Lines\n    vertical_lines = sorted(vertical_lines, key=lambda line: line[0])  # Sort by x-coordinate\n\n    # Step 4: Split Columns\n    column_files = []\n    for i in range(len(vertical_lines) - 1):\n        x1, _, x2, _ = vertical_lines[i]\n        x3, _, x4, _ = vertical_lines[i + 1]\n\n        # Adjust for line offset\n        x1 += line_offset\n        x2 = x3 - line_offset\n\n        # Ensure valid column width\n        if x1 >= x2 or (x2 - x1) < min_column_width:\n            print(f\"Skipping invalid or small column: x1={x1}, x2={x2}, width={x2 - x1}\")\n            continue\n\n        # Crop and save column\n        cropped_column = image[:, x1:x2]\n        column_filename = os.path.join(output_folder, f\"{column_prefix}_col_{i+1}.png\")\n        cv2.imwrite(column_filename, cropped_column)\n        column_files.append(column_filename)\n\n    return column_files\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Usage\nrow_image_path = \"/kaggle/working/cropped_row_temp/153_0_row_30.png\"\noutput_folder = \"/kaggle/working/cropped_cells\"\nos.makedirs(output_folder, exist_ok=True)\n\ncolumn_prefix = \"column\"  # Matches the prefix in training_tags.txt\nboard_files = detect_and_split_columns_debug(\n    row_image_path=row_image_path,\n    output_folder=output_folder,\n    column_prefix=column_prefix\n    \n#    min_width=250,         # 1.5 inches at 96 DPI\n#    min_height=50,         # 0.25 inches at 96 DPI\n#    max_region_ratio=0.3,  # Exclude regions larger than 90% of the image\n#    padding=5             # Extra padding\n)\n\nprint(\"Extracted chessboards:\", board_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -r /kaggle/working/cropped_cells","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_folder = \"/kaggle/working/cropped_row_temp\"\nos.makedirs(output_folder, exist_ok=True)\n!cp /kaggle/working/cropped_rows/153_0_row_30.png $output_folder/153_0_row_30.png","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below function just prints the DPI of images","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\ndef print_image_info(image_path):\n    \"\"\"\n    Print resolution and DPI of a PNG image.\n\n    Args:\n        image_path (str): Path to the PNG file.\n    \"\"\"\n    with Image.open(image_path) as img:\n        # Get resolution\n        width, height = img.size\n        print(f\"Resolution: {width} x {height} pixels\")\n\n        # Get DPI (if available)\n        dpi = img.info.get('dpi', None)\n        if dpi:\n            print(f\"DPI: {dpi[0]} x {dpi[1]} (horizontal x vertical)\")\n        else:\n            print(\"DPI information not available in this image.\")\n\n# Example Usage\nimage_path = \"/kaggle/input/chess-dataset-notation/data/001_0.png\"\nprint_image_info(image_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from paddleocr import PaddleOCR, draw_ocr\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Initialize PaddleOCR\nocr = PaddleOCR(use_angle_cls=True, lang='en')  # Initialize with English language\n\n# Path to the PNG file\nimage_path = '/kaggle/input/chess-dataset-notation/data/012_0.png'\n\n# Perform OCR on the image\nresult = ocr.ocr(image_path, cls=True)\n\n# Display OCR Results\nfor line in result[0]:\n    print(f\"Detected Text: {line[1][0]}, Confidence: {line[1][1]}\")\n\n# Visualize OCR Results on the Image\nimage = Image.open(image_path).convert('RGB')\nboxes = [line[0] for line in result[0]]\ntxts = [line[1][0] for line in result[0]]\nscores = [line[1][1] for line in result[0]]\n\n# Draw the OCR results on the image\nannotated_image = draw_ocr(image, boxes, txts, scores, font_path='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf')  # Add path to TTF font if necessary\nplt.imshow(annotated_image)\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install paddlepaddle\n!pip install paddleocr\nfrom paddleocr import PaddleOCR, draw_ocr\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageOps, ImageFont\nimport numpy as np\nimport os\n\n# Path to your valid readings list\nvalid_readings_path = '/kaggle/input/aarrrrr/present.txt'\n\n\n# Initialize PaddleOCR with the custom dictionary\nocr = PaddleOCR(\n    use_angle_cls=True,\n    lang='en'\n)\n\n# Path to the PNG file\nimage_path = '/kaggle/input/chess-dataset-notation/data/001_0.png'\n\n# Perform OCR on the image\nresult = ocr.ocr(image_path, cls=True)\n\n# Sort OCR results by reading order\ndef sort_by_reading_order(results):\n    # Sort by vertical position first (top-to-bottom)\n    results = sorted(results, key=lambda x: x[0][0][1])  # x[0][0][1] is the y-coordinate of the top-left corner\n    # Further sort by horizontal position (left-to-right) within the same row\n    sorted_results = []\n    row_threshold = 10  # Threshold to group lines based on y-coordinates\n    current_row = [results[0]]\n    \n    for i in range(1, len(results)):\n        if abs(results[i][0][0][1] - current_row[-1][0][0][1]) < row_threshold:\n            current_row.append(results[i])\n        else:\n            sorted_results.extend(sorted(current_row, key=lambda x: x[0][0][0]))  # Sort by x-coordinate\n            current_row = [results[i]]\n    \n    sorted_results.extend(sorted(current_row, key=lambda x: x[0][0][0]))\n    return sorted_results\n\nsorted_results = sort_by_reading_order(result[0])\n\n# Display OCR Results\nprint(\"Sorted OCR Results:\")\nfor line in sorted_results:\n    print(f\"Detected Text: {line[1][0]}, Confidence: {line[1][1]}\")\n\n# Visualize OCR Results on the Image\nimage = Image.open(image_path).convert('RGB')\nboxes = [line[0] for line in sorted_results]\ntxts = [line[1][0] for line in sorted_results]\nscores = [line[1][1] for line in sorted_results]\n\n# Use a valid font path\nfont_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n\n# Draw the OCR results on the image\nannotated_image = draw_ocr(image, boxes)#, txts, scores, font_path=font_path)\n\n# Convert NumPy array to PIL Image\nannotated_image_pil = Image.fromarray(np.uint8(annotated_image))\n\n# Save the annotated image\noutput_path = '/kaggle/working/annotated_image_with_dict.png'\nannotated_image_pil.save(output_path)\nprint(f\"Annotated image saved at {output_path}\")\n\n# Display the saved image\nplt.imshow(annotated_image_pil)\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(annotated_image)\noutput_path = '/kaggle/working/annotated_image.png'  # Adjust path if needed\nannotated_image_pil = Image.fromarray(np.uint8(annotated_image))\nannotated_image_pil.save(output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls  \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Create a directory to store the models\nos.makedirs('inference', exist_ok=True)\n\n# Download models\n!wget -P inference https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\n!wget -P inference https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\n!wget -P inference https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/ch_ppstructure_mobile_v2.0_SLANet_infer.tar\n\n# Extract the models\n!tar xf inference/ch_PP-OCRv3_det_infer.tar -C inference\n!tar xf inference/ch_PP-OCRv3_rec_infer.tar -C inference\n!tar xf inference/ch_ppstructure_mobile_v2.0_SLANet_infer.tar -C inference\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from paddleocr import PPStructure, draw_structure_result, save_structure_res\nfrom PIL import Image\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np \n\n# Initialize the table recognition pipeline\ntable_engine = PPStructure(table=True)\n\n# Path to your image\nimage_path = '/kaggle/input/chess-dataset-notation/data/012_0.png'\n\n# Run table recognition\nresults = table_engine(image_path)\n\n# Save and visualize the results\nsave_folder = '/kaggle/working/'\nsave_structure_res(results, save_folder, os.path.basename(image_path).split('.')[0])\n\n# Load and display the annotated image\nimg = Image.open(image_path).convert('RGB')\ndraw_img = draw_structure_result(img, results, font_path=\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\")\n\n# Convert to BGR for display with OpenCV\ncv2_draw_img = cv2.cvtColor(np.array(draw_img), cv2.COLOR_RGB2BGR)\nplt.figure(figsize=(12, 12))\nplt.imshow(cv2_draw_img)\nplt.axis('off')\nplt.show()\n\n# Save the annotated image\nannotated_image_path = '/kaggle/working/annotated_image.png'\nannotated_image_pil = Image.fromarray(np.uint8(draw_img))\nannotated_image_pil.save(annotated_image_path)\nprint(f\"Annotated table image saved at: {annotated_image_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install premailer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_labels(label_file):\n    image_labels = {}\n    with open(label_file, 'r') as file:\n        for line in file:\n            parts = line.strip().split()\n            image_name = parts[0]\n            label = parts[1]\n            base_name = \"_\".join(image_name.split('_')[:2])\n            if base_name not in image_labels:\n                image_labels[base_name] = []\n            image_labels[base_name].append(label)\n    return image_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimage_labels_dict = load_labels('/kaggle/input/chess-dataset-notation/data/training_tags.txt')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"   def _parse_labels(self, label_file):\n        image_labels = []\n        image_labels_dict = load_labels(label_file)\n   import os\n        for base_name, moves in image_labels_dict.items():\n            base_name = base_name.split('_')[0]\n            image_path_0 = os.path.join(self.image_folder, f\"{base_name}_0.png\")\n            image_path_1 = os.path.join(self.image_folder, f\"{base_name}_1.png\")\n            if os.path.exists(image_path_0):\n                image_labels.append((image_path_0, moves))\n            elif os.path.exists(image_path_1):\n                image_labels.append((image_path_1, moves))\n        return image_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"define dataset class","metadata":{}},{"cell_type":"code","source":"import os\nfrom torchvision import models\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport cv2\nfrom torchmetrics import Accuracy, Precision, Recall, F1Score, AveragePrecision\nfrom sklearn.metrics import average_precision_score\nimport torch.optim as optim\n\nclass ChessNotationDataset(Dataset):\n    def __init__(self, image_folder, label_file, move_to_idx, transform=None):\n        self.image_folder = image_folder\n        self.move_to_idx = move_to_idx\n        self.transform = transform\n        self.image_labels = self._parse_labels(label_file)\n\n    def _parse_labels(self, label_file):\n        image_labels = []\n        image_labels_dict = load_labels(label_file)\n        for base_name, moves in image_labels_dict.items():\n            base_name = base_name.split('_')[0]\n            image_path_0 = os.path.join(self.image_folder, f\"{base_name}_0.png\")\n            image_path_1 = os.path.join(self.image_folder, f\"{base_name}_1.png\")\n            if os.path.exists(image_path_0):\n                image_labels.append((image_path_0, moves))\n            elif os.path.exists(image_path_1):\n                image_labels.append((image_path_1, moves))\n        return image_labels\n\n    def __len__(self):\n        return len(self.image_labels)\n\n    def __getitem__(self, idx):\n        image_path, moves = self.image_labels[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        image = np.array(image)\n        \n        if image.ndim == 2:\n            image = np.expand_dims(image, axis=-1)\n            image = np.repeat(image, 3, axis=-1)\n        \n        if image.dtype != np.uint8:\n            image = image.astype(np.uint8)\n\n        # Resize image to 299x299 (for InceptionV3)\n        image = cv2.resize(image, (299, 299))\n\n        # Convert image to tensor\n        image = torch.from_numpy(image.transpose((2, 0, 1))).float() / 255.0\n\n        # Convert the move labels to a tensor\n        labels = convert_labels_to_tensor(moves, self.move_to_idx)\n        return image, labels\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"functions","metadata":{}},{"cell_type":"code","source":"def create_move_to_idx(label_file):\n    move_to_idx = {}\n    idx = 0\n    with open(label_file, 'r') as file:\n        for line in file:\n            move = line.strip()\n            move_key = move.lower()\n            if move_key not in move_to_idx:\n                move_to_idx[move_key] = idx\n                idx += 1\n    return move_to_idx\n\n# Convert labels to tensor\ndef convert_labels_to_tensor(moves, move_to_idx):\n    return torch.tensor([move_to_idx[move.lower()] for move in moves], dtype=torch.long)\n\ndef load_labels(label_file):\n    image_labels = {}\n    with open(label_file, 'r') as file:\n        for line in file:\n            parts = line.strip().split()\n            image_name = parts[0]\n            label = parts[1]\n            base_name = \"_\".join(image_name.split('_')[:2])\n            if base_name not in image_labels:\n                image_labels[base_name] = []\n            image_labels[base_name].append(label)\n    return image_labels\n    \ndef custom_collate_fn(batch):\n    images = torch.stack([item[0] for item in batch])\n    \n    labels = [item[1] for item in batch]\n    lengths = torch.tensor([len(label) for label in labels])\n    max_len = max(lengths)\n    \n    padded_labels = []\n    for label in labels:\n        padding = torch.full((max_len - len(label),), padding_idx, dtype=torch.long)\n        padded_label = torch.cat([label, padding])\n        padded_labels.append(padded_label)\n    \n    labels = torch.stack(padded_labels)\n    \n    return images, labels, lengths, max_len","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize dataset and model\npadding_idx = -1\nimage_folder = \"/kaggle/input/chess-dataset-notation/data\"\nlabel_file = \"/kaggle/input/training-tags-file/training_tags.txt\"\nmove_to_idx = create_move_to_idx(\"/kaggle/input/ripchessgm/san_strings_with_symbols.txt\")\ndataset = ChessNotationDataset(\n    image_folder=image_folder,\n    label_file=label_file,\n    move_to_idx=move_to_idx,\n    transform=None\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"dataset count:\",len(move_to_idx))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install paddlepaddle\n!pip install paddleocr\n\nimport paddle\nimport paddle.nn as nn\nfrom paddleocr.ppocr.modeling.backbones import MobileNetV3\nfrom paddleocr.ppocr.modeling.necks import SequenceEncoder\n\nclass ChessSequenceModel(nn.Layer):\n    def __init__(self, num_classes, max_seq_length=100, hidden_size=512, num_layers=1):\n        super(ChessSequenceModel, self).__init__()\n        self.max_seq_length = max_seq_length\n\n        # Use PaddleOCR's MobileNetV3 backbone for feature extraction\n        self.backbone = MobileNetV3(scale=0.5, model_name='small')\n        backbone_output_size = 576  # Output feature size of MobileNetV3-small\n\n        # Add LSTM layer to handle sequences\n        self.lstm = nn.LSTM(\n            input_size=backbone_output_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            time_major=False  # Set this to True if you pass sequences in (time, batch, feature) format\n        )\n\n        # Final output layer\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        # Pass input through the backbone (feature extractor)\n        features = self.backbone(x)\n        \n        # Flatten the features for sequence modeling\n        batch_size, channels, height, width = features.shape\n        features = features.reshape([batch_size, width, -1])  # Convert to (batch, sequence, feature)\n\n        # Pass features through the LSTM\n        lstm_out, _ = self.lstm(features)\n\n        # Use the final LSTM output for classification\n        output = self.fc(lstm_out[:, -1, :])  # Use the last output of LSTM\n\n        return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# !conda create -n clean_env python=3.10 scikit-learn=1.2.2\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-22T22:24:07.721448Z","iopub.execute_input":"2024-11-22T22:24:07.721935Z"}}},{"cell_type":"code","source":"!conda create -n clean_env python=3.10 scikit-learn=1.2.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport pytesseract\nimport os\n\n# Specify the path to the image\nimg_path = '/kaggle/input/aarrrrr/IMG-0886.jpg'\n\n# Open the image using Pillow\nimg = Image.open(img_path)\n\n# Perform OCR\ntext = pytesseract.image_to_string(img)\n\n# Print the extracted text\nprint(\"Extracted Text:\\n\", text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"try to remove horizontal lines","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\n\ndef split_by_horizontal_lines_sequential(image_path, output_folder, row_prefix, line_offset=2, min_row_height=10):\n    \"\"\"\n    Split an image into rows based on horizontal lines, excluding lines themselves, with sequential numbering.\n\n    Args:\n        image_path (str): Path to the PNG file.\n        output_folder (str): Folder to save cropped rows.\n        row_prefix (str): Prefix for naming cropped regions.\n        line_offset (int): Number of pixels to offset the crop region to exclude lines.\n        min_row_height (int): Minimum height in pixels for a row to be considered valid.\n\n    Returns:\n        list of str: Filenames of cropped rows.\n    \"\"\"\n    # Load image\n    image = cv2.imread(image_path)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Edge detection to highlight lines\n    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n\n    # Detect lines using Hough Transform\n    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi / 180, threshold=100,\n                            minLineLength=100, maxLineGap=10)\n\n    # Extract horizontal lines (angle ~ 0 degrees)\n    horizontal_lines = []\n    for line in lines:\n        x1, y1, x2, y2 = line[0]\n        if abs(y1 - y2) < 5:  # Check if the line is horizontal\n            horizontal_lines.append((x1, y1, x2, y2))\n\n    # Create a mask for inpainting\n    mask = np.zeros_like(gray)\n    for x1, y1, x2, y2 in horizontal_lines:\n        cv2.line(mask, (x1, y1), (x2, y2), 255, thickness=2)\n\n    # Inpaint the image\n    inpainted_image = cv2.inpaint(image, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n\n    # Sort horizontal lines by their y-coordinates\n    horizontal_lines = sorted(horizontal_lines, key=lambda line: line[1])\n\n    # Split the image into rows using horizontal lines\n    row_files = []\n    row_counter = 1  # Sequential counter for rows\n    for i in range(len(horizontal_lines) - 1):\n        _, y1, _, _ = horizontal_lines[i]\n        _, y2, _, _ = horizontal_lines[i + 1]\n\n        # Exclude lines by adding an offset\n        y1 = y1 + line_offset  # Start just below the current line\n        y2 = y2 - line_offset  # End just above the next line\n\n        # Validate y-coordinates\n        y1 = max(0, y1)\n        y2 = min(gray.shape[0], y2)\n        if y1 >= y2 or (y2 - y1) < min_row_height:  # Check minimum row height\n            print(f\"Skipping invalid or small row: y1={y1}, y2={y2}, height={y2 - y1}\")\n            continue\n\n        # Crop the region\n        cropped_row = inpainted_image[y1:y2, :]\n        if cropped_row.size == 0:\n            print(f\"Empty cropped row for y1={y1}, y2={y2}, skipping...\")\n            continue\n\n        # Save the cropped row with sequential numbering\n        row_filename = f\"{row_prefix}_row_{row_counter}.png\"\n        cv2.imwrite(os.path.join(output_folder, row_filename), cropped_row)\n        row_files.append(row_filename)\n        row_counter += 1  # Increment the row counter\n\n    # Save a debug image with horizontal lines drawn\n    debug_image = image.copy()\n    for x1, y1, x2, y2 in horizontal_lines:\n        cv2.line(debug_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n    debug_filename = os.path.join(output_folder, f\"{row_prefix}_debug.png\")\n    cv2.imwrite(debug_filename, debug_image)\n\n    return row_files\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Usage\nimage_path = \"/kaggle/input/chess-dataset-notation/data/153_0.png\"\noutput_folder = \"/kaggle/working/cropped_rows\"\nos.makedirs(output_folder, exist_ok=True)\n\nrow_prefix = \"153_0\"  # Matches the prefix in training_tags.txt\nboard_files = split_by_horizontal_lines_sequential(\n    image_path=image_path,\n    output_folder=output_folder,\n    row_prefix=row_prefix\n    \n#    min_width=250,         # 1.5 inches at 96 DPI\n#    min_height=50,         # 0.25 inches at 96 DPI\n#    max_region_ratio=0.3,  # Exclude regions larger than 90% of the image\n#    padding=5             # Extra padding\n)\n\nprint(\"Extracted chessboards:\", board_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from paddleocr import PaddleOCR, draw_ocr\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nimport numpy as np\nimport json\n\n# Initialize PaddleOCR\nocr = PaddleOCR(use_angle_cls=True, lang='en')\n\n# Path to the PNG file\nimage_path = '/kaggle/input/chess-dataset-notation/data/001_0.png'\n\n# Perform OCR on the image\nresult = ocr.ocr(image_path, cls=True)\n\n# Sort OCR results by reading order\ndef sort_by_reading_order(results):\n    # Sort by vertical position first (top-to-bottom)\n    results = sorted(results, key=lambda x: x[0][0][1])  # x[0][0][1] is the y-coordinate of the top-left corner\n    # Further sort by horizontal position (left-to-right) within the same row\n    sorted_results = []\n    row_threshold = 10  # Threshold to group lines based on y-coordinates\n    current_row = [results[0]]\n    \n    for i in range(1, len(results)):\n        if abs(results[i][0][0][1] - current_row[-1][0][0][1]) < row_threshold:\n            current_row.append(results[i])\n        else:\n            sorted_results.extend(sorted(current_row, key=lambda x: x[0][0][0]))  # Sort by x-coordinate\n            current_row = [results[i]]\n    \n    sorted_results.extend(sorted(current_row, key=lambda x: x[0][0][0]))\n    return sorted_results\n\nsorted_results = sort_by_reading_order(result[0])\n\n# Display OCR Results\nprint(\"Sorted OCR Results:\")\nfor idx, line in enumerate(sorted_results, start=1):\n    print(f\"Box {idx}: Detected Text: {line[1][0]}, Confidence: {line[1][1]}\")\n\n# Function to check if a box should be skipped based on X-coordinates\ndef is_in_excluded_x_range(box, column_threshold=50, exclusion_range=(730, 790)):\n    x_coords = [point[0] for point in box]\n    min_x, max_x = min(x_coords), max(x_coords)\n    # Skip if box is in the first column or within the excluded range\n    return min_x <= column_threshold or (min_x >= exclusion_range[0] and max_x <= exclusion_range[1])\n\n# Prepare training labels and filter boxes\ntraining_labels = []\nfiltered_boxes = []\nfiltered_texts = []\nfor idx, line in enumerate(sorted_results, start=1):\n    box = line[0]\n    text = line[1][0]\n    if not is_in_excluded_x_range(box):  # Skip boxes in excluded X-coordinate ranges\n        training_labels.append({\n            \"coordinates\": box,\n            \"text\": text,\n            \"box_id\": idx\n        })\n        filtered_boxes.append((box, idx))  # Keep box and index for drawing\n        filtered_texts.append(text)  # Keep text for mapping\n\n# Save training labels as a JSON file\noutput_label_path = '/kaggle/working/training_labels.json'\nwith open(output_label_path, 'w') as f:\n    json.dump(training_labels, f, indent=4)\nprint(f\"Training labels saved at {output_label_path}\")\n\n# Draw filtered OCR results on the image with numbered boxes\nimage = Image.open(image_path).convert('RGB')\ndraw = ImageDraw.Draw(image)\n\n# Use a valid font path for numbering\nfont_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\nfont = ImageFont.truetype(font_path, size=20)\n\nfor box, idx in filtered_boxes:\n    draw.polygon([tuple(point) for point in box], outline=\"red\", width=2)  # Draw box\n    # Draw the box number at the top-left corner of the box\n    draw.text((box[0][0], box[0][1] - 20), str(idx), fill=\"blue\", font=font)\n\n# Save the annotated image with numbered boxes\noutput_image_path = '/kaggle/working/annotated_image_filtered.png'\nimage.save(output_image_path)\nprint(f\"Annotated image saved at {output_image_path}\")\n\n# Display the saved image\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}